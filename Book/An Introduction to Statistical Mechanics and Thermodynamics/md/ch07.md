# 7

# Classical Gases: Ideal and Otherwise

All models are wrong, but some are useful.

George Box, British statistician (1919-2013)

In Chapter 2 we introduced Boltzmann's 1877 definition of the entropy in terms of the logarithm of a probability for a composite system. We assumed that the positions and momenta of the particles in a classical ideal gas were independent random variables, so that we could examine their contributions to the entropy independently. In Chapter 4 we calculated the contributions from the positions of the particles, and in Chapter 6 we calculated the contributions from the momenta of the particles. Now we are in a position to put them together to find the total entropy of a composite system.

# 7.1 Entropy of a Composite System of Classical Ideal Gases

We have seen that the contributions to the total entropy of the composite system from the positions of the particles in eq. (4.15) and their momenta in eq. (6.32) simply add to give the total entropy. Since both the configurational and energy-dependent contributions to the entropy of the composite system are separable into contributions from each subsystem, the total entropy is also separable. It is easier (and requires less space) to look at the expression for the entropy of a single system at this point, rather than the composite system needed for the derivations in Chapters 4 and 6,

$$
S _ {j} \left(E _ {j}, V _ {j}, N _ {j}\right) = k N _ {j} \left[ \ln \left(\frac {E _ {j} ^ {3 N _ {j} / 2 - 1}}{\left(3 N _ {j} / 2\right) !}\right) + \ln \left(\frac {V _ {j} ^ {N _ {j}}}{N _ {j} !}\right) + X ^ {\prime} \right]. \tag {7.1}
$$

This equation contains two constants that are still arbitrary at this point:  $k$ , which will be discussed in Chapter 8, and  $X'$ , which we will discuss further in this chapter.

For any macroscopic system—that is, any system with more than about  $10^{10}$  particles—Stirling's approximation is excellent. We can also replace  $3N_{j} / 2 - 1$  with  $3N_{j} / 2$ , which has a relative error of only  $3 / 2N_{j}$ . The result is the entropy of a classical ideal gas,

$$
S _ {j} (E _ {j}, V _ {j}, N _ {j}) = k N _ {j} \left[ \frac {3}{2} \ln \left(\frac {E _ {j}}{N _ {j}}\right) + \ln \left(\frac {V _ {j}}{N _ {j}}\right) + X \right], \tag {7.2}
$$

under the approximation that the widths of the energy- and particle-number-distributions are neglected. The constant  $X = X' + 1$  comes from the replacement of  $N_{j}!$  by Stirling's approximation.

The value of  $X$  in eq. (7.2) is completely arbitrary if we restrict ourselves to classical mechanics. However, there is a traditional value chosen for  $X$ ,

$$
X = \frac {3}{2} \ln \left(\frac {4 \pi m}{3 h ^ {2}}\right) + \frac {5}{2}. \tag {7.3}
$$

The constant  $h$  in this expression is Planck's constant, taken from quantum mechanics. The presence of Planck's constant in eq. (7.3) makes it obvious that this value of  $X$  has nothing to do with classical mechanics. It is determined by solving for the entropy of a quantum mechanical gas—for which the additive constant does have a meaning—and taking the classical limit. We will carry out this procedure in Chapters 27, 28, and 29.

# 7.2 Equilibrium Conditions for the Ideal Gas

By definition, the entropy of any composite system should be a maximum at equilibrium. Eq. (7.2) satisfies that condition with respect to both energy and particle number. We will show this explicitly below—first for equilibrium with respect to energy, and then with respect to particle number.

# 7.2.1 Equilibrium with Respect to Energy

We want to confirm that the expression we have derived for the entropy predicts the correct equilibrium values for the energies of two subsystems. To do that, consider an experiment on a composite system with fixed subvolumes,  $V_{j}$  and  $V_{k}$ , containing  $N_{j}$  and  $N_{k}$  particles respectively. The total energy of the particles in the composite system is  $E_{T,jk}$ . There is a diathermal wall separating the two subvolumes, so that they can exchange energy.

We want to find the maximum of the entropy to confirm that it occurs at the equilibrium values of  $E_{j}$  and  $E_{k} = E_{T,jk} - E_{j}$ . We find the maximum in the usual way, by setting the partial derivative with respect to  $E_{j}$  equal to zero,

$$
\begin{array}{l} \frac {\partial}{\partial E _ {j}} S \left(E _ {j}, V _ {j}, N _ {j}; E _ {T, j k} - E _ {j}, V _ {k}, N _ {k}\right) = \frac {\partial}{\partial E _ {j}} \left[ S _ {j} \left(E _ {j}, V _ {j}, N _ {j}\right) + S _ {k} \left(E _ {T, j k} - E _ {j}, V _ {k}, N _ {k}\right) \right] \\ = \frac {\partial}{\partial E _ {j}} S _ {j} \left(E _ {j}, V _ {j}, N _ {j}\right) + \frac {\partial E _ {k}}{\partial E _ {j}} \frac {\partial}{\partial E _ {k}} S _ {k} \left(E _ {k}, V _ {k}, N _ {k}\right) \\ = \frac {\partial}{\partial E _ {j}} S _ {j} (E _ {j}, V _ {j}, N _ {j}) - \frac {\partial}{\partial E _ {k}} S _ {k} (E _ {k}, V _ {k}, N _ {k}) \\ = 0. \tag {7.4} \\ \end{array}
$$

This gives us an equilibrium condition that will play a very important role in the development of thermodynamics,

$$
\frac {\partial S _ {j}}{\partial E _ {j}} = \frac {\partial S _ {k}}{\partial E _ {k}}. \tag {7.5}
$$

Since the partial derivative of the entropy with respect to energy is

$$
\begin{array}{l} \frac {\partial S _ {j}}{\partial E _ {j}} = k N _ {j} \frac {\partial}{\partial E _ {j}} \left[ \frac {3}{2} \ln \left(\frac {E _ {j}}{N _ {j}}\right) + \ln \left(\frac {V _ {j}}{N _ {j}}\right) + X \right] \\ = \frac {3}{2} k N _ {j} \frac {\partial}{\partial E _ {j}} \left[ \ln E _ {j} - \ln N _ {j} \right] \\ = \frac {3 k N _ {j}}{2 E _ {j}} \tag {7.6} \\ \end{array}
$$

the equilibrium condition is predicted to be

$$
\frac {E _ {j}}{N _ {j}} = \frac {E _ {k}}{N _ {k}} \tag {7.7}
$$

which is the same equilibrium condition found in eq. (6.26).

# 7.2.2 Equilibrium with Respect to the Number of Particles

We now demonstrate that our expression for the entropy predicts the correct equilibrium values of the numbers of particles in each subsystem. Consider an experiment on a composite system with fixed subvolumes,  $V_{j}$  and  $V_{k}$ . The total number of particles is  $N_{T,jk}$ , but there is a hole in the wall between the two subvolumes, so that they can exchange both energy and particles. The total energy of the particles in the composite system is  $E_{T,jk}$ .

Since the two subsystems can exchange particles, they can certainly exchange energy. The derivation in the previous subsection is still valid, so that the entropy is correctly maximized at the equilibrium value of the energy; that is, the same average energy per particle in the two subsystems.

Now we want to find the maximum of the entropy with respect to  $N_{j}$  to confirm that it occurs at the equilibrium values of  $N_{j}$  and  $N_{k} = N_{T,jk} - N_{j}$ . We find the maximum in the usual way, by setting the partial derivative with respect to  $N_{j}$  equal to zero,

$$
\begin{array}{l} \frac {\partial}{\partial N _ {j}} S (E _ {j}, V _ {j}, N _ {j}; E _ {k}, V _ {j}, N _ {T, j k} - N _ {j}) = \frac {\partial}{\partial N _ {j}} \left[ S _ {j} (E _ {j}, V _ {j}, N _ {j}) + S _ {k} (E _ {k}, V _ {k}, N _ {T, j k} - N _ {j}) \right] \\ = \frac {\partial}{\partial N _ {j}} S _ {j} (E _ {j}, V _ {j}, N _ {j}) + \frac {\partial N _ {k}}{\partial N _ {j}} \frac {\partial}{\partial N _ {k}} S _ {k} (E _ {k}, V _ {k}, N _ {k}) \\ = \frac {\partial}{\partial N _ {j}} S _ {j} (E _ {j}, V _ {j}, N _ {j}) - \frac {\partial}{\partial N _ {k}} S _ {k} (E _ {k}, V _ {k}, N _ {k}) \\ = 0. \tag {7.8} \\ \end{array}
$$

This gives us another equilibrium condition, in analogy to eq. (7.5), which will also be important in thermodynamics,

$$
\frac {\partial S _ {j}}{\partial N _ {j}} = \frac {\partial S _ {k}}{\partial N _ {k}}. \tag {7.9}
$$

The partial derivative of the entropy with respect to  $N_{j}$  is rather complicated:

$$
\begin{array}{l} \frac {\partial S}{\partial N _ {j}} = \frac {\partial}{\partial N _ {j}} \left[ k N _ {j} \left[ \frac {3}{2} \ln \left(\frac {E _ {j}}{N _ {j}}\right) + \ln \left(\frac {V _ {j}}{N _ {j}}\right) + X \right] \right] \\ = k \left[ \frac {3}{2} \ln \left(\frac {E _ {j}}{N _ {j}}\right) + \ln \left(\frac {V _ {j}}{N _ {j}}\right) + X \right] + k N _ {j} \frac {\partial}{\partial N _ {j}} \left[ \frac {3}{2} \ln \left(\frac {E _ {j}}{N _ {j}}\right) + \ln \left(\frac {V _ {j}}{N _ {j}}\right) + X \right] \\ = k \left[ \frac {3}{2} \ln \left(\frac {E _ {j}}{N _ {j}}\right) + \ln \left(\frac {V _ {j}}{N _ {j}}\right) + X \right] + k N _ {j} \left[ - \frac {3}{2 N _ {j}} - \frac {1}{N _ {j}} \right] \\ = k \left[ \frac {3}{2} \ln \left(\frac {E _ {j}}{N _ {j}}\right) + \ln \left(\frac {V _ {j}}{N _ {j}}\right) + X \right] - \frac {5}{2} k. \tag {7.10} \\ \end{array}
$$

The condition for equilibrium is to set the partial derivatives with respect to the number of particles equal in the two subvolumes,

$$
k \left[ \frac {3}{2} \ln \left(\frac {E _ {j}}{N _ {j}}\right) + \ln \left(\frac {V _ {j}}{N _ {j}}\right) + X \right] - \frac {5}{2} k = k \left[ \frac {3}{2} \ln \left(\frac {E _ {k}}{N _ {k}}\right) + \ln \left(\frac {V _ {k}}{N _ {k}}\right) + X \right] - \frac {5}{2} k. (7. 1 1)
$$

Because of the equilibrium with respect to energy, we also have eq. (7.7),

$$
\frac {E _ {j}}{N _ {j}} = \frac {E _ {k}}{N _ {k}}. \tag {7.12}
$$

Combining eqs. (7.11) and eqs. (7.12), we find the equation for equilibrium with respect to  $N_{j}$ ,

$$
\frac {N _ {j}}{V _ {j}} = \frac {N _ {k}}{V _ {k}}. \tag {7.13}
$$

As expected from eq. (4.8), the number of particles per unit volume is the same in both subsystems.

# 7.3 The Volume-Dependence of the Entropy

An interesting and important feature of the expression we have derived for the entropy of the classical ideal gas in eq. (7.2) is that it also correctly predicts the results for a kind of experiment that we have not yet discussed.

Consider a cylinder, closed at both ends, containing an ideal gas. There is a freely moving partition, called a piston, that separates the gas into two subsystems. The piston is made of a diathermal material, so it can transfer energy between the two systems, but it is impervious to the particles. There are  $N_{j}$  particles on one side of the piston and  $N_{k}$  particles on the other. Since the piston can move freely, the volumes of the subsystems can change, although the total volume,  $V_{T,jk} = V_{j} + V_{k}$ , is fixed.

Because the piston can conduct heat, the average energy per particle will be the same on both sides of the piston, as derived in Chapter 6:

$$
\frac {\left\langle E _ {j} \right\rangle}{N _ {j}} = \frac {\left\langle E _ {k} \right\rangle}{N _ {k}}. \tag {7.14}
$$

The maximum of the entropy with respect to the position of the piston is then found by setting the derivative of the total entropy with respect to  $V_{j}$  equal to zero:

$$
\begin{array}{l} \frac {\partial}{\partial V _ {j}} S \left(E _ {j}, V _ {j}, N _ {j}; E _ {k}, V _ {T, j k} - V _ {j}, N _ {k}\right) = \frac {\partial}{\partial V _ {j}} \left[ S _ {j} \left(E _ {j}, V _ {j}, N _ {j}\right) + S _ {k} \left(E _ {k}, V _ {T, j k} - V _ {j}, N _ {k}\right) \right] \\ = \frac {\partial}{\partial V _ {j}} S _ {j} \left(E _ {j}, V _ {j}, N _ {j}\right) + \frac {\partial V _ {k}}{\partial V _ {j}} \frac {\partial}{\partial V _ {k}} S _ {k} \left(E _ {k}, V _ {k}, N _ {k}\right) \\ = \frac {\partial}{\partial V _ {j}} S _ {j} \left(E _ {j}, V _ {j}, N _ {j}\right) - \frac {\partial}{\partial V _ {k}} S _ {k} \left(E _ {k}, V _ {k}, N _ {k}\right) \\ = 0. \tag {7.15} \\ \end{array}
$$

This gives us the equilibrium condition in a familiar form,

$$
\frac {\partial S _ {j}}{\partial V _ {j}} = \frac {\partial S _ {k}}{\partial V _ {k}}. \tag {7.16}
$$

Since the partial derivative of the entropy with respect to volume is

$$
\frac {\partial S _ {j}}{\partial V _ {j}} = k N \frac {\partial}{\partial V _ {j}} \left[ \frac {3}{2} \ln \left(\frac {E _ {j}}{N _ {j}}\right) + \ln \left(\frac {V _ {j}}{N _ {j}}\right) + X \right]
$$

$$
\begin{array}{l} = k N _ {j} \frac {\partial}{\partial V _ {j}} \ln \left(\frac {V _ {j}}{N _ {j}}\right) \\ = \frac {k N _ {j}}{V _ {j}} \tag {7.17} \\ \end{array}
$$

and the equilibrium condition that the particle density is the same in both subsystems is correctly predicted,

$$
\frac {N _ {j}}{V _ {j}} = \frac {N _ {k}}{V _ {k}}. \tag {7.18}
$$

How did this happen? It is nice to know that it is true, but why should the maximum of the entropy with respect to the position of the piston produce the correct equilibrium volumes? The answer can be found by calculating the probability distribution for the position of the piston.

Under the usual assumption that all configurations are equally likely (subject to the constraints), the joint probability density for the positions of the particles and the position of the piston should be a constant, which we denote as  $Y$

$$
P \left(V _ {j}, \{\vec {r} _ {1, i} | i = 1, \dots , N _ {j} \}, \{\vec {r} _ {2, j} | j = 1, \dots , N _ {k} \}\right) = Y. \tag {7.19}
$$

To find the marginal probability distribution  $P(V_{j})$ , we simply integrate over the positions of every particle, noting that  $N_{j}$  particles are restricted to volume  $V_{j}$ , and  $N_{k}$  particles are restricted to volume  $V_{k}$ :

$$
\begin{array}{l} P \left(V _ {j}\right) = \int_ {p _ {j}, p _ {k}} P \left(V _ {j}, \{\vec {r} _ {1, i} | i = 1, \dots , N _ {j} \}, \{\vec {r} _ {2, j} | j = 1, \dots , N _ {k} \}\right) d ^ {3 N _ {j}} r _ {j} d ^ {3 N _ {j}} r _ {k} \\ = Y V _ {j} ^ {N _ {j}} V _ {k} ^ {N _ {k}}. \tag {7.20} \\ \end{array}
$$

To find the maximum of  $P(V_{j})$  under the condition that  $V_{k} = V_{T,jk} - V_{j}$ , we differentiate with respect to  $V_{j}$ , and set it equal to zero,

$$
\frac {\partial}{\partial V _ {j}} P \left(V _ {j}\right) = Y N _ {j} V _ {j} ^ {N _ {j} - 1} V _ {k} ^ {N _ {k}} - Y N _ {k} V _ {j} ^ {N _ {j}} \left(V _ {T, j k} - V _ {j}\right) ^ {N _ {k} - 1} = 0. \tag {7.21}
$$

Solving eq. (7.21), we find the hoped-for result:

$$
\frac {N _ {j}}{V _ {j}} = \frac {N _ {k}}{V _ {k}}. \tag {7.22}
$$

The key point is that the logarithm of the volume dependence of  $P(V_{j})$  is exactly the same as that of the entropy. In every case, defining the entropy as the logarithm of the probability (to within additive and multiplicative constants) gives the correct answer.

# 7.4 Asymmetric Pistons

The derivation of the entropy assumes that the total volume is conserved when two systems are connected with a moveable piston. This will be the case if the pistons have the same cross section in each system that they connect, although that cross section may be different for the pistons linking two other systems.

However, this is not the most general experimental situation. A piston connecting systems  $j$  and  $k$  might have a different cross-sectional area of  $A_{j}$  on one side and a cross-sectional area of  $A_{k}$  on the other side. This would lead to a change in the total volume if the piston moves.

We can show that the same form of the entropy will correctly predict equilibrium for the case of pistons with differing cross sections for each system, although the form of the equilibrium equations will be modified. The demonstration that this is the case is much simpler if we have the interpretation of the partial derivatives with respect to energy (giving the temperature) and volume (giving the pressure). Therefore, we will postpone the demonstration until Section 8.7.

# 7.5 Indistinguishable Particles

In Chapter 4 we calculated the configurational entropy of a classical ideal gas of distinguishable particles. On the other hand, from quantum mechanics we know that atoms and molecules of the same type are indistinguishable. It is therefore interesting to ask in what way the probability distribution, and consequently the entropy and other properties of the composite system, would change if we were to assume that the particles were indistinguishable. It turns out that nothing changes.

For the following derivation, recall that particles are regarded as distinguishable if and only if the exchange of two particles produces a different microscopic state. If the exchange of two particles does not produce a distinct microscopic state, the particles are indistinguishable.

The question is how to modify the derivation of eq. (4.4) in Chapter 4 to account for indistinguishability.

The first consequence of indistinguishability is that the binomial coefficient introduced to account for the permutations of the particles between the two subsystems must be eliminated. Since the particles are indistinguishable, the number of microscopic states generated by exchanging particles is equal to 1.

The second consequence is that the factors  $(V_{j} / V)^{N_{j}}(1 - V_{j} / V)^{N_{T,jk} - N_{j}}$  in eq. (4.4) must also be modified.

Consider our basic assumption that the probability density for the positions of the particles must be a constant, with the value of the constant determined by the condition that the probability density be normalized to 1.

For  $N_{j}$  distinguishable particles in a three-dimensional volume  $V_{j}$ , the probability density is clearly  $V_{j}^{-N_{j}}$ . The probability of finding a specific set of  $N_{j}$  particles in subvolume  $V_{j}$  and the rest in subvolume  $V_{k}$  is therefore  $V_{j}^{N_{j}}V_{k}^{N_{T,jk} - N_{j}}V_{T,jk}^{-N_{T,jk}}$ .

However, the calculation for indistinguishable particles is somewhat different. We cannot label individual particles in a way that distinguishes between them. We can, however, label the particles on the basis of their positions. If two particles are exchanged, their labels would also be exchanged, and the state would be unchanged. A simple way of achieving this is to introduce a three-dimensional Cartesian coordinate system and label particles in order of their x-coordinates; that is, for any microscopic state, particles are labeled so that  $x_{j,i} < x_{j,i+1}$  in subsystem  $j$  and  $x_{k,\ell} < x_{k,\ell+1}$  in subsystem  $k$ . For simplicity, we will assume that the subsystems are rectangular, with edges parallel to the coordinate axes. The lengths of the subsystems in the  $x$ -direction are  $L_j$  and  $L_k$ , and the corresponding areas are  $A_j$  and  $A_k$ , so that the volumes are given by  $V_j = A_j L_j$  and  $V_k = A_k L_k$ .

We will follow the same procedure established in Chapters 4 and 6 and assume the probability distribution in coordinate space to be a constant, which we will denote by  $Y(N_{T,jk}, V_{T,jk})$ ,

$$
P (\{\vec {r} _ {j, i} \}, \{\vec {r} _ {k \ell} \}) = Y (N _ {T, j k}, V _ {T, j k}). \tag {7.23}
$$

The probability of finding  $N_{j}$  particles in subvolume  $V_{j}$  and  $N_{k} = N_{T,jk} - N_{j}$  in subvolume  $V_{k}$  is found by integrating over coordinate space. The integrals in the  $y$ - and  $z$ -directions just give factors of the cross-sectional areas of the two subsystems. The integrals over the  $x$ -coordinates must be carried out consistently with the labeling condition that  $x_{j,i} < x_{j,i+1}$  and  $x_{k,\ell} < x_{k,\ell+1}$ :

$$
\begin{array}{l} P \left(N _ {j}, N _ {k}\right) = Y \left(N _ {T, j k}, V _ {T, j k}\right) A _ {j} ^ {N _ {j}} A _ {k} ^ {N _ {k}} \int_ {0} ^ {L _ {j}} d x _ {j, N _ {j}} \int_ {0} ^ {x _ {j, N _ {j}}} d x _ {j, N _ {j} - 1} \dots \int_ {0} ^ {x _ {k}} d x _ {j, 1} \\ \times \int_ {0} ^ {L _ {k}} d x _ {k, N _ {k}} ^ {\prime} \int_ {0} ^ {x _ {k, N _ {k}} ^ {\prime}} d x _ {k, N _ {k} - 1} ^ {\prime} \dots \int_ {0} ^ {x _ {k} ^ {\prime}} d x _ {k, 1} ^ {\prime}. \tag {7.24} \\ \end{array}
$$

I have used primes to indicate the integrals over the  $x$ -coordinates in subsystem  $k$ .

The integrals in eq. (7.24) are easily carried out by iteration,

$$
P (N _ {j}, N _ {k}) = Y (N _ {T, j k}, V _ {T, j k}) \left(\frac {V _ {j} ^ {N _ {j}}}{N _ {j} !}\right) \left(\frac {V _ {k} ^ {N _ {k}}}{N _ {k} !}\right). \qquad \qquad (7. 2 5)
$$

This gives the probability distribution of identical particles, except for the determination of the normalization constant,  $Y(N_{T,jk},V_{T,jk})$ .

It is clear that the dependence of  $P(N_{j},N_{k})$  on its arguments for indistinguishable particles in eq. (7.25) is exactly the same as that for distinguishable particles in eq. (4.4). This means that we can determine the value of the normalization constant by comparison with the normalized binomial distribution in eq. (3.47):

$$
Y \left(N _ {T, j k}, V _ {T, j k}\right) = \frac {N _ {T , j k} !}{V _ {T , j k} ^ {N _ {T , j k}}}. \tag {7.26}
$$

The full probability distribution for identical particles is found by substituting eq. (7.26) in eq. (7.25),

$$
P \left(N _ {j}, N _ {k}\right) = \frac {N _ {T , j k} !}{V _ {T , j k} ^ {N _ {T , j k}}} \left(\frac {V _ {j} ^ {N _ {j}}}{N _ {j} !}\right) \left(\frac {V _ {k} ^ {N _ {k}}}{N _ {k} !}\right). \tag {7.27}
$$

This expression is identical to the probability distribution for distinguishable particles in eq. (4.4).

Since the entropy is given by the logarithm of the probability distribution, the entropy of a classical gas is exactly the same for distinguishable and indistinguishable particles.

The result that the entropy is exactly the same for classical systems with distinguishable and indistinguishable particles might be surprising to some readers, since most of the literature for the past century has claimed that they were different. My claim that the two models have the same entropy depends crucially on relating the entropy to the probability distribution. However, I think we might take as a general principle that two models with identical properties should have the same entropy. Since the probability distributions are the same, their properties are the same. It is hard to see how any definition that results in different entropies for two models with identical properties can be defended.

# 7.6 Entropy of a Composite System of Interacting Particles

Up to this point we have considered only an ideal gas, in which there are no interactions between particles. In real gases, particles do interact: particles can attract each other to form liquids and they can repel each other to make the liquid difficult to compress.

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-16/ed77a26b-2172-4711-82a3-4634df49002c/42a83b694cf6bfd0ebba0e3f7bfa1adb9c1476a1155c8c123b7e84808f3e7597.jpg)  
Fig. 7.1 Schematic plot of typical interatomic potential. The potential,  $\varphi (r)$ , is significantly different from zero only over a few nanometers.

Fig. 7.1 shows a typical interatomic potential, which is significantly different from zero only over very short distances.

Even though interatomic potentials are very short-ranged, they can have important effects, including the formation of liquid and solid phases, with phase transitions between them. We will return to this topic in Chapters 19 and 31.

To generalize our analysis of the entropy of the classical ideal gas to include interacting particles, it is useful to go back and re-derive the entropy of the ideal gas, without taking advantage of the independence of the positions and the momenta. Our basic assumption is still that of a uniform probability distribution in phase space, consistent with the constraints on the composite system.

# 7.6.1 The Ideal Gas Revisited

We begin with our assumption from Section 1.4 that the probability distribution in phase space is uniform, subject to the constraints. The physical constraints are:

1.  $N_{j}$  particles are in subsystem  $j$  and  $N_{k}$  in subsystem  $k$ .  
2. Subsystem  $j$  has an energy  $E_{j}$  and subsystem  $k$  has an energy  $E_{k}$ .

Instead of doing the integrals separately for the configurations and the momenta, we can write the final result for  $P(E_{j},V_{j},N_{j};E_{k},V_{k},N_{k})$  as an integral over all of phase space, with Dirac delta functions imposing the energy constraints as before. This does not simplify matters for the ideal gas, but it will make the generalization to real gases easier.

Calculating the probability distribution for the energies, volumes, and numbers of particles of each subsystem is straightforward, but the expression becomes a little

unwieldy. While it is great fun for a teacher to write it across two or three blackboards so that students can see it as a whole, the constraints of a book make it somewhat more difficult to write out completely. To make it manageable, we will use the same compact notation that we used earlier for the integrals over phase space. The  $3N_{\alpha}$ -dimensional integral over the positions of the particles in subsystem  $\alpha$  will be indicated as  $\int dq_{\alpha}$ , and similarly for the other variables.

Write the Hamiltonian of the subsystem  $\alpha$  as

$$
H _ {\alpha} \left(q _ {\alpha}, p _ {\alpha}\right) = \sum_ {i = 1} ^ {3 N _ {\alpha}} \frac {\left| \vec {p} _ {\alpha , i} \right| ^ {2}}{2 m} \tag {7.28}
$$

where  $\alpha = j$  or  $k$  to indicate the subsystems. To make the notation more compact we will write  $H_{\alpha}$  to indicate  $H_{\alpha}(q_{\alpha},p_{\alpha})$ .

The marginal probability distribution for the energies, volumes, and number of particles of each subsystem is then found by integrating over all space:

$$
P \left(E _ {j}, V _ {j}, N _ {j}; E _ {k}, V _ {k}, N _ {k}\right) =
$$

$$
\frac {N _ {T , j k} !}{N _ {j} ! N _ {k} !} \frac {\int d q _ {j} \int d p _ {j} \int d q _ {k} \int d p _ {k} \delta \left(E _ {j} - H _ {j}\right) \delta \left(E _ {k} - H _ {k}\right)}{\int d q \int d p \delta \left(E _ {T , j k} - H _ {T , j k}\right)}. \tag {7.29}
$$

Because the particles in the two subsystems do not interact with each other, eq. (7.29) can be written in a more compact form by introducing a function

$$
\Omega_ {\alpha} \left(E _ {\alpha}, V _ {\alpha}, N _ {\alpha}\right) = \frac {1}{h ^ {3 N _ {\alpha}} N _ {\alpha} !} \int d q _ {\alpha} \int d p _ {\alpha} \delta \left(E _ {\alpha} - H _ {\alpha} \left(q _ {\alpha}, p _ {\alpha}\right)\right) \tag {7.30}
$$

where we have written  $H_{\alpha}(q_{\alpha},p_{\alpha})$  for clarity. Note that we have included a factor of  $h^{-3N_{\alpha}}$  in the expression for  $\Omega_{\alpha}$ . This factor is not required classically, but it is allowed since we can always multiply the right side of eq. (7.29) by  $1 = h^{N_T,j_k} / h^{N_j}h^{N_k}$ . We will see later that this simple modification produces consistency between the classical result for the ideal gas and the classical limit of a quantum ideal gas.

With this notation, eq. (7.29) becomes

$$
P (E _ {j}, V _ {j}, N _ {j}; E _ {k}, V _ {k}, N _ {k}) = \frac {\Omega_ {j} (E _ {j} , V _ {j} , N _ {j}) \Omega_ {k} (E _ {k} , V _ {k} , N _ {k})}{\Omega \left(E _ {T , j k} , V _ {T , j k} , N _ {T , j k}\right)}. \tag {7.31}
$$

Taking the logarithm of the probability distribution in eq. (7.31), in analogy to eq. (4.17), we find the expression for the entropy of the composite system,

$$
\begin{array}{l} S _ {t o t} \left(E _ {j}, V _ {j}, N _ {j}; E _ {k}, V _ {k}, N _ {k}\right) = k \ln \left[ P \left(E _ {j}, V _ {j}, N _ {j}; E _ {k}, V _ {k}, N _ {k}\right) \right] + S \left(E _ {T, j k}, V _ {T, j k}, N _ {T, j k}\right) \\ = S _ {j} \left(E _ {j}, V _ {j}, N _ {j}\right) + S _ {k} \left(E _ {k}, V _ {k}, N _ {k}\right). \tag {7.32} \\ \end{array}
$$

(We have neglected to include an arbitrary additive constant because it has no physical consequences.)

The entropy of an isolated system is given by the logarithm of  $\Omega$ ,

$$
S (E, V, N) = k \ln \Omega (E, V, N), \tag {7.33}
$$

where  $E$ ,  $V$ , and  $N$  are again generic variables, which can represent properties of subsystem  $j$ , subsystem  $k$ , or the entire composite system.

Eq. (7.32) shows that the total entropy of an ideal gas is an additive. It will be left as an exercise to show that this expression for the entropy of a classical ideal gas is identical to that in eq. (7.2)—including the value of the constant  $X$  given in eq. (7.3).

Note that we have introduced a new constant,  $h$ , into eq. (7.30). It is easy to confirm that this expression for the probability distribution is still equivalent to eq. (7.29) for any value of  $h$ , so that this new constant is completely arbitrary, and therefore meaningless within classical statistical mechanics. It has been introduced solely for the purpose of ensuring consistency with quantum statistical mechanics. It is a remarkable fact that the choice of  $h$  as Planck's constant produces agreement with quantum statistical mechanics in the classical limit. This probably seems rather mysterious at this point, but it will become clear when we return to this question in Chapters 27, 28, and 29.

# 7.6.2 Generalizing the Entropy to Interacting Particles

We do not need to make any changes in our fundamental assumptions about the probability distribution in phase space to find the entropy of a system with interacting particles. We still assume a uniform probability distribution in phase space, subject to the physical constraints of our composite system. The only difference is that the energy now includes interaction terms. However, because it is possible for a molecule in one subsystem to interact with a molecule in the other subsystem, the separability of the entropy of the composite system into the sum of the entropies for each subsystem is no longer trivial.

First write the Hamiltonian of the composite system composed of subsystems  $j$  and  $k$  in terms of sums over the particles,

$$
H _ {T, j k} (q, p) = \sum_ {i = 1} ^ {N _ {T, j k}} \frac {\left| \vec {p} _ {i} \right| ^ {2}}{2 m} + \sum_ {i = 1} ^ {N _ {T, j k}} \sum_ {i ^ {\prime} > i} ^ {N _ {T, j k}} \phi \left(\vec {r} _ {i}, \vec {r} _ {i ^ {\prime}}\right) \tag {7.34}
$$

or

$$
H _ {T, j k} (q, p) = \sum_ {i = 1} ^ {N _ {j}} \frac {| \vec {p} _ {j , i} | ^ {2}}{2 m} + \sum_ {\ell = 1} ^ {N _ {k}} \frac {| \vec {p} _ {k , \ell} | ^ {2}}{2 m}
$$

$$
\begin{array}{l} + \sum_ {i = 1} ^ {N _ {j}} \sum_ {i ^ {\prime} > i} ^ {N _ {j}} \phi (\vec {r} _ {j, i}, \vec {r} _ {j, i ^ {\prime}}) + \sum_ {\ell = 1} ^ {N _ {k}} \sum_ {\ell^ {\prime} > \ell} ^ {N _ {k}} \phi (\vec {r} _ {k, \ell}, \vec {r} _ {k, \ell^ {\prime}}) \\ + \sum_ {i = 1} ^ {N _ {j}} \sum_ {\ell = 1} ^ {N _ {k}} \phi (\vec {r} _ {j, i}, \vec {r} _ {k, \ell}). \tag {7.35} \\ \end{array}
$$

This equation is written two ways. First, as a single, composite system, and then broken into pieces, representing the subsystems. The last term on the right in eq. (7.35) represents the interactions between particles in different subsystems. This is the term that causes difficulties in separating the entropy of the composite system into a sum of entropies of the subsystems.

We can define the Hamiltonian of subsystem  $j$ :

$$
H _ {j} \left(q _ {j}, p _ {j}\right) = \sum_ {i = 1} ^ {N _ {j}} \frac {\left| \vec {p} _ {j , i} \right| ^ {2}}{2 m} + \sum_ {i = 1} ^ {N _ {j}} \sum_ {i ^ {\prime} > i} ^ {N _ {j}} \phi \left(\vec {r} _ {j, i}, \vec {r} _ {j, i ^ {\prime}}\right). \tag {7.36}
$$

The limit of  $i' > i$  in the second sum over positions is there to prevent double counting. Similarly, we can define the Hamiltonian of subsystem  $k$ :

$$
H _ {k} \left(q _ {k}, p _ {k}\right) = \sum_ {\ell = 1} ^ {N _ {k}} \frac {\left| \vec {p} _ {k , \ell} \right| ^ {2}}{2 m} + \sum_ {\ell = 1} ^ {N _ {k}} \sum_ {\ell^ {\prime} > \ell} ^ {N _ {k}} \phi \left(\vec {r} _ {k, \ell}, \vec {r} _ {k, \ell^ {\prime}}\right). \tag {7.37}
$$

However, because of the interaction term linking the two subsystems,

$$
H _ {j k} \left(q _ {j}, q _ {k}\right) = \sum_ {j _ {1} = 1} ^ {N _ {j}} \sum_ {\ell = 1} ^ {N _ {k}} \phi \left(\vec {r} _ {j, i}, \vec {r} _ {k, \ell}\right) \tag {7.38}
$$

the total Hamiltonian is not just the sum of  $H_{j}$  and  $H_{k}$ :

$$
H _ {T, j k} (q, p) = H _ {j} \left(q _ {j}, p _ {j}\right) + H _ {k} \left(q _ {k}, p _ {k}\right) + H _ {j k} \left(q _ {j}, q _ {k}\right). \tag {7.39}
$$

Fortunately, the range of the interactions between molecules in most materials (or between particles in colloids) is short in comparison with the size of the system. Therefore, the contributions to  $H_{jk}$  are only significant for molecules in different subsystems that are close to each other. Since the range of molecular interactions is only a few nanometers, this means that only pairs of molecules very close to the interface between the two subsystems contribute at all. The energy corresponding to these direct interactions between molecules in different systems is therefore proportional to the size of the interface, which scales as the surface area in each system, or  $V_{\alpha}^{2/3}$ .

In terms of the number of particles, the contribution of the direct interactions to molecules subsystem  $\alpha$  with a different subsystem is proportional to  $N_{\alpha}^{2/3}$ . As a fraction of all molecules in subsystem  $\alpha$ , this is  $N_{\alpha}^{2/3} / N_{\alpha} = N_{\alpha}^{-1/3}$ . However, the fraction of molecules close to the interface in another subsystem,  $\alpha'$ , is proportional to  $N_{\alpha'}^{-1/3}$ . This makes the total effect of the interface of the order of  $N_{\alpha}^{-1/3}N_{\alpha'}^{-1/3}$ . If  $N_{\alpha}$  and  $N_{\alpha'}$  are both of the order of  $10^{21}$ , then the interaction term is of the order of  $N_{\alpha}^{-1/3}N_{\alpha'}^{-1/3} \approx 10^{-7}10^{-7} = 10^{-14}$ . Therefore, neglecting the interactions between molecules in different subsystems is a very good approximation.

Neglecting the term  $H_{jk}$  in eq. (7.39), we can again write the joint probability in terms of the product of two terms as in eq. (7.29). The difference is that now the Hamiltonians only include the interactions between particles within the same system,

$$
P \left(E _ {j}, V _ {j}, N _ {j}; E _ {k}, V _ {k}, N _ {k}\right) =
$$

$$
\frac {N _ {T , j k} !}{N _ {j} ! N _ {k} !} \frac {\int d q _ {j} \int d p _ {j} \int d q _ {k} \int d p _ {k} \delta \left(E _ {j} - H _ {j}\right) \delta \left(E _ {k} - H _ {k}\right)}{\int d q \int d p \delta \left(E _ {T , j k} - H _ {T , j k}\right)} \tag {7.40}
$$

or

$$
P \left(E _ {j}, V _ {j}, N _ {j}; E _ {k}, V _ {k}, N _ {k}\right) =
$$

$$
\frac {N _ {T , j k} !}{N _ {j} ! N _ {k} !} \frac {\int d q _ {j} \int d p _ {j} \delta \left(E _ {j} - H _ {j}\right) \int d q _ {k} \int d p _ {k} \delta \left(E _ {k} - H _ {k}\right)}{\int d q \int d p \delta \left(E _ {T , j k} - H _ {T , j k}\right)}. \tag {7.41}
$$

The factors in eq. (7.41) can be made explicit by introducing a generalized  $\Omega$  function,

$$
\Omega_ {\alpha} \left(E _ {\alpha}, V _ {\alpha}, N _ {\alpha}\right) = \frac {1}{h ^ {3 N _ {\alpha}} N _ {\alpha} !} \int d q _ {\alpha} \int d p _ {\alpha} \delta \left(E _ {\alpha} - H _ {\alpha}\right) \tag {7.42}
$$

and writing

$$
P \left(E _ {j}, V _ {j}, N _ {j}; E _ {k}, V _ {k}, N _ {k}\right) = \frac {\Omega_ {j} \left(E _ {j} , V _ {j} , N _ {j}\right) \Omega_ {k} \left(E _ {k} , V _ {k} , N _ {k}\right)}{\Omega \left(E _ {T , j k} , V _ {T , j k} , N _ {T , j k}\right)}. \tag {7.43}
$$

As before, the subscript  $\alpha$  can take on the values of  $j$  or  $k$  to represent a subsystem, or be replaced by  $T,jk$  to represent the full system. The parameter  $h$  is still completely arbitrary within classical statistical mechanics. As mentioned above,  $h$  will be identified as Planck's constant in Chapters 27, 28, and 29 to ensure consistency with the classical limit of the corresponding quantum system.

Now that we have expressed the probability distribution for interacting classical particles as a product of terms, we can define the entropy by taking the logarithm and multiplying it by a constant,  $k$ . The result is formally the same as eq. (7.32) for ideal gases,

$$
\begin{array}{l} S _ {t o t} (E _ {j}, V _ {j}, N _ {j}; E _ {k}, V _ {k}, N _ {k}) = k \ln \left[ P (E _ {j}, V _ {j}, N _ {j}; E _ {k}, V _ {k}, N _ {k}) \right] + S (E _ {T, j k}, V _ {T, j k}, N _ {T, j k}) \\ = S _ {j} \left(E _ {j}, V _ {j}, N _ {j}\right) + S _ {k} \left(E _ {k}, V _ {k}, N _ {k}\right). \tag {7.44} \\ \end{array}
$$

The entropy terms of individual systems are again given by logarithm of the  $\Omega$  functions,

$$
\begin{array}{l} S _ {\alpha} \left(E _ {\alpha}, V _ {\alpha}, N _ {\alpha}\right) = k \ln \Omega_ {\alpha} \left(E _ {\alpha}, V _ {\alpha}, N _ {\alpha}\right) \tag {7.45} \\ = k \ln \left[ \frac {1}{h ^ {3 N _ {\alpha}} N _ {\alpha} !} \int d q _ {\alpha} \int d p _ {\alpha} \delta (E _ {\alpha} - H _ {\alpha}) \right]. \\ \end{array}
$$

Now the Hamiltonians  $H_{j}$  and  $H_{k}$  include the intra-system interactions.

# 7.7 The Second Law of Thermodynamics

As was the case for ideal gases, the probability distributions for energy, volume, and number of particles in composite systems of interacting particles are very narrow—the width is generally proportional to the square root of the number of particles. The relative width of the probability distributions will therefore again be much smaller than experimental errors. For systems of interacting particles, the observed values of the energy, volume, and number of particles will agree with the location of the maxima of the corresponding probability distributions to within the accuracy of the experiment.

A comparison of eq. (7.41) for the probabilities in a compound system of interacting particles and eq. (7.43) for the entropy of the composite system shows that the location of the maximum of the entropy gives the equilibrium values of the quantity of interest when the corresponding internal constraints are released. This is an important result for two reasons.

First, it allows us to find the equilibrium values of the energy, volume, or number of particles for any experimental situation, once the entropy is known.

Second, if the entropy is maximized for unconstrained equilibrium, it necessarily follows that any constrained macroscopic state of the composite system must have an equal or lower total entropy. This is the Second Law of Thermodynamics.

If the composite system is in any constrained macroscopic state—that is, if there are any constraints on the energy, volume, or number of particles in any subsystem—the total entropy of the composite system cannot decrease if those constraints are removed.

# 7.8 Equilibrium between Subsystems

For a composite system composed of two general, classical subsystems, the total entropy is the sum of the entropies of the subsystems, as indicated in eq. (7.44),

$$
S _ {t o t} (E _ {j}, V _ {j}, N _ {j}; E _ {k}, V _ {k}, N _ {k}) = S _ {j} (E _ {j}, V _ {j}, N _ {j}) + S _ {k} (E _ {k}, V _ {k}, N _ {k}). \tag {7.46}
$$

Since the total entropy is maximized upon release of a constraint, we can determine the new equilibrium parameters by setting the appropriate partial derivative of the total entropy equal to zero.

For example, consider a situation in which the volumes and numbers of particles are held fixed in the subsystems, but the subsystems are connected by a diathermal wall. Equilibrium is found from the partial derivative with respect to  $E_{j}$ , with  $E_{k} = E_{T,jk} - E_{j}$ :

$$
\begin{array}{l} \frac {\partial}{\partial E _ {j}} S _ {t o t} (E _ {j}, V _ {j}, N _ {j}; E _ {T, j k} - E _ {j}, V _ {k}, N _ {k}) \\ = \frac {\partial}{\partial E _ {j}} S _ {j} \left(E _ {j}, V _ {j}, N _ {j}\right) + \frac {\partial}{\partial E _ {j}} S _ {k} \left(E _ {T, j k} - E _ {j}, V _ {k}, N _ {k}\right) = 0. \tag {7.47} \\ \end{array}
$$

Since  $E_{k} = E_{T,jk} - E_{j}$ , it must be that  $\partial E_k / \partial E_j = -1$ , and we can rewrite eq. (7.47) in a symmetric form:

$$
\frac {\partial}{\partial E _ {j}} S _ {j} \left(E _ {j}, V _ {j}, N _ {j}\right) = - \frac {\partial E _ {k}}{\partial E _ {j}} \frac {\partial}{\partial E _ {k}} S _ {k} \left(E _ {k}, V _ {k}, N _ {k}\right) \tag {7.48}
$$

or

$$
\frac {\partial S _ {j}}{\partial E _ {j}} = \frac {\partial S _ {k}}{\partial E _ {k}}. \tag {7.49}
$$

Eq. (7.49) is important. Beyond giving us an algorithm for finding the equilibrium value of  $E_{j}$ , it tells us that the condition of equilibrium is that a derivative of the entropy with respect to the energy has the same value for two subsystems in thermal contact.

Similar arguments show that the condition for equilibrium with respect to volume when two subsystems are separated by a moveable piston is given by

$$
\frac {\partial S _ {j}}{\partial V _ {j}} = \frac {\partial S _ {k}}{\partial V _ {k}}, \tag {7.50}
$$

and equilibrium with respect to particle number when a hole is made in the wall between two subsystems is given by

$$
\frac {\partial S _ {j}}{\partial N _ {j}} = \frac {\partial S _ {k}}{\partial N _ {k}}. \tag {7.51}
$$

These last three equations provide both a means of solving problems and the basis for establishing the connection between statistical mechanics and thermodynamics. The explicit relationships between these derivatives and familiar quantities like temperature and pressure will be the subject of the following chapter. However, the first and arguably the most important consequence of these equations is presented in the next section.

# 7.9 The Zeroth Law of Thermodynamics

An immediate consequence of eqs. (7.49), (7.50), and (7.51) is that if two systems are each in equilibrium with a third system, they must also be in equilibrium with each other.

Assume systems  $j$  and  $k$  are in equilibrium with respect to energy, then eq. (7.49) holds. If systems  $k$  and  $\ell$  are also in equilibrium with respect to energy, then we have a similar equation for the partial derivatives of  $S_{k}$  and  $S_{\ell}$  with respect to energy,

$$
\frac {\partial S _ {j}}{\partial E _ {j}} = \frac {\partial S _ {k}}{\partial E _ {k}} = \frac {\partial S _ {\ell}}{\partial E _ {\ell}}. \tag {7.52}
$$

Since the partial derivatives with respect to energy must be the same for systems  $j$  and  $\ell$ , they must be in equilibrium with respect to energy.

Clearly, this argument is equally valid for the volume or the number of particles. We therefore have a general principle that if two systems are each in equilibrium with a third system, then they are in equilibrium with each other.

This is the Zeroth Law of Thermodynamics. It is valid both for equilibrium when the subsystems in thermal contact with each other, and for potential equilibrium if the subsystems are isolated from each other, but eq. (7.52) holds.

The numbering of the laws of thermodynamics might seem rather strange—especially the 'zeroth'. The need to state it explicitly as a law of thermodynamics was not recognized during the nineteenth century when thermodynamics was being developed, perhaps because it is so fundamental that it seemed obvious. When it was declared a law of thermodynamics, the numbers for the other laws had long been well established. Ralph H. Fowler (British physicist and astronomer, 1889-1944) is credited with calling it the Zeroth Law to place it in the leading position. At this point in the book we have encountered only the Second and Zeroth Laws of Thermodynamics. The First and Third Laws are yet to come.

# 7.10 Problems

# PROBLEM 7.1

# General forms for the entropy

We have derived the entropy of a classical ideal gas, which satisfies all the postulates of thermodynamics (except the Nernst postulate, which applies to quantum systems). However, when interactions are included, the entropy could be a very different function of  $U$ ,  $V$ , and  $N$ . Here are some functions that are candidates for being the entropy of some system. [In each case,  $A$  is a positive constant.]

1. Which of the following equations for the entropy satisfy the postulates of thermodynamics (except the Nernst postulate)?

1.  $S = A(UVN)^{1 / 3}$  
2.  $S = A\left(\frac{NU}{V}\right)^{2 / 3}$  
3.  $S = A\left(\frac{UV}{N}\right)$  
4.  $S = A\left(\frac{V^3}{NU}\right)$

2. For each of the valid forms of the entropy in the previous part of the problem, find the three equations of state.

# PROBLEM 7.2

The 'traditional' expression for the entropy of an ideal gas of distinguishable particles

It is claimed in many textbooks on statistical mechanics that the correct entropy for a classical ideal gas of distinguishable particles differs from what we derived in class. The traditional expression is

$$
S (E, V, N) = k _ {k} N \left[ \frac {3}{2} \ln \left(\frac {E}{N}\right) + \ln (V) + X \right].
$$

Assume that you have two systems,  $j$  and  $k$ , with  $N_{j}$  and  $N_{k}$  particles and  $V_{j}$  and  $V_{k}$  volumes respectively, which are both described by an entropy of this form.

Consider an experiment in which the volumes are fixed, but a hole is made in the wall separating the two subsystems.

Calculate the location of the maximum of the total traditional entropy to determine its prediction for the equilibrium distribution of the energy and the particles between the two systems.

Naturally, your answer should depend on the relative sizes of the fixed volumes,  $V_{j}$  and  $V_{k}$ .