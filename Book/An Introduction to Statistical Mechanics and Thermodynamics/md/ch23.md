# 23

# Quantum Ensembles

If you are not completely confused by quantum mechanics, you do not understand it. John Archibald Wheeler, American theoretical physicist (1911-2008)

In Part IV of this book we will extend the application of statistical ideas to quantum mechanical systems. Most of the results we will derive have counterparts in classical statistical mechanics, but some important features will be quite new.

The differences between classical and quantum statistical mechanics are all based on the differing concepts of a microscopic 'state'. While the classical microscopic state (specified by a point in phase space) determines the exact position and momentum of every particle, the quantum mechanical state determines neither; quantum states can only provide probability distributions for observable quantities. This feature has the important consequence that quantum statistical mechanics involves two different kinds of probability, while classical statistical mechanics involves only one.

Classical and quantum statistical mechanics both require the assignment of probabilities to the possible microscopic states of a system. The calculation of all quantities involves averages over such probability distributions in both theories.

Quantum statistical mechanics further requires averages over the probability distributions that are obtained from each individual microscopic state.

We will begin this chapter by recalling the basic equations of quantum mechanics, assuming that the reader is already familiar with the material. If any part of the discussion seems mysterious, it would be advisable to consult a textbook on quantum mechanics.

After a review of the basic ideas we will discuss the special features that distinguish quantum statistical mechanics from the classical theory.

Subsequent chapters will develop the theory of quantum statistical mechanics and apply it to models that demonstrate the significant differences between the predictions of quantum and classical statistical mechanics. We will show that even simple harmonic oscillators and ideal gases have dramatically different properties in quantum and classical theories. Since the real world ultimately obeys quantum mechanics, quantum statistical mechanics is essential to obtain agreement with experimental results.

# 23.1 Basic Quantum Mechanics

The most fundamental characteristic of quantum mechanics is that the microscopic state of a system is described by a wave function, rather than a point in phase space. For a simple, one-particle system, the wave function can be represented by a complex function of space and time,  $\psi (\vec{r},t)$ , which contains all possible information about the particle.

Even if the wave function is known exactly, the result of a measurement of the position of the particle can only be predicted in terms of probabilities. For example, the absolute value of the square of the wave function,  $|\psi(\vec{r},t)|^2$ , gives the probability density for the result of a measurement of the position of a particle. (After the measurement, of course, the wave function of the particle would change.) Only the average position of the particle and the various moments of its probability distribution can be calculated from the wave function.

The momentum,  $\vec{p}$ , is represented by a vector operator rather than a real vector,

$$
\vec {p} = - i \hbar \vec {\nabla} = - i \hbar \left(\frac {\partial}{\partial x}, \frac {\partial}{\partial y}, \frac {\partial}{\partial z}\right). \tag {23.1}
$$

As is the case for the position of the particle, the momentum does not have a fixed value. The expectation value (average value) of the momentum is given by an integral over the wave function. This can be denoted as

$$
\langle \psi | \vec {p} | \psi \rangle = - i \hbar \int \psi^ {*} (\vec {r}, t) \vec {\nabla} \psi (\vec {r}, t) d ^ {3} r. \tag {23.2}
$$

In eq. (23.2),  $\psi^{*}$  denotes the complex conjugate of the function  $\psi$ , and we have introduced the 'bra',  $\langle \psi | = \psi^{*}(\vec{r},t)$ , and 'ket',  $|\psi\rangle = \psi(\vec{r},t)$  notation. The integral is over the allowed volume.

The kinetic and potential energy are represented by a Hamiltonian,

$$
H (\vec {r}, \vec {p}) = H (\vec {r}, - i \hbar \vec {\nabla}) = \frac {\vec {p} \cdot \vec {p}}{2 m} + V (\vec {r}) = \frac {- \hbar^ {2}}{2 m} \nabla^ {2} + V (\vec {r}). (2 3. 3)
$$

In this equation,

$$
\nabla^ {2} = \vec {\nabla} \cdot \vec {\nabla} = \frac {\partial^ {2}}{\partial x ^ {2}} + \frac {\partial^ {2}}{\partial y ^ {2}} + \frac {\partial^ {2}}{\partial z ^ {2}}. \tag {23.4}
$$

The wave function must satisfy the Schrödinger equation, named after the Austrian physicist Erwin Schrödinger (1887-1961, Nobel Prize 1933),

$$
i \hbar \frac {\partial}{\partial t} \psi (\vec {r}, t) = H (\vec {r}, - i \hbar \vec {\nabla}) \psi (\vec {r}, t). \tag {23.5}
$$

# 23.2 Energy Eigenstates

There are certain special quantum states that can be written as a product of a function of position times a function of time,

$$
\psi (\vec {r}, t) = \psi (\vec {r}) f (t). \tag {23.6}
$$

For these states, the Schrödinger equation can be written in a form that separates the functions of time and space:

$$
i \hbar \frac {1}{f (t)} \frac {\partial f (t)}{\partial t} = \frac {1}{\psi (\vec {r})} H (\vec {r}, - i \vec {\hbar \nabla}) \psi (\vec {r}). \tag {23.7}
$$

Since the left side of eq. (23.7) depends only on time and the right side depends only on space, they must both be equal to a constant, which we will denote as  $E$ . For these special states, the Schrödinger equation separates into two equations:

$$
\frac {\partial f (t)}{\partial t} = \frac {- i E}{\hbar} f (t), \tag {23.8}
$$

and

$$
H (\vec {r}, - i \hbar \vec {\nabla}) \psi (\vec {r}) = E \psi (\vec {r}). \tag {23.9}
$$

Since the expectation value of  $H$  is just

$$
\langle H \rangle = E, \tag {23.10}
$$

we can identify  $E$  as the energy of the state.

Eq. (23.8) can be easily solved as a function of time,

$$
f (t) = \exp \left(- i \frac {E}{\hbar} t\right). \tag {23.11}
$$

In general, eq. (23.9) only has solutions for particular values of  $E$ , which are the only observable values of the energy. These special values are known as energy eigenvalues, and the corresponding wave functions are known as eigenfunctions. Eq. (23.9) is called an eigenvalue equation. The eigenfunctions and eigenvalues can be identified by a quantum 'number'  $n$ , which is really a set of  $d$  numbers that describe the eigenstate for a wave function in a  $d$ -dimensional system.

Including the quantum number explicitly, the eigenvalue equation can be written as

$$
H \psi_ {n} (\vec {r}) = E _ {n} \psi_ {n} (\vec {r}), \tag {23.12}
$$

or

$$
H | n \rangle = E _ {n} | n \rangle , \tag {23.13}
$$

where we have suppressed the explicit dependence on  $\vec{r}$  in the second form of the eigenvalue equation.  $E_{n}$  is called the energy eigenvalue. It is possible for distinct eigenstates to have the same energy eigenvalue, in which case the states are called 'degenerate'.

The time-dependence of an eigenstate therefore has the form

$$
\psi_ {n} (\vec {r}, t) = \psi_ {n} (\vec {r}) \exp \left(- \frac {i E}{\hbar} t\right). \tag {23.14}
$$

Since the expectation value of any operator is constant in time for eigenstates, they are also called 'stationary states'.

It is standard procedure to normalize the eigenfunctions

$$
\langle n | n \rangle = \int \psi_ {n} ^ {*} (\vec {r}, t) \psi_ {n} (\vec {r}, t) d ^ {3} r = 1. \tag {23.15}
$$

In this equation, we have again used the 'bra-ket' notation  $(\psi_{n}(\vec{r},t) = |n\rangle)$ .

It is easily proved that for  $E_{n} \neq E_{m}$ ,  $\langle n|m\rangle = 0$ . It is also straightforward to choose eigenfunctions in such a way that for  $n \neq m$ ,  $\langle n|m\rangle = 0$ , even when  $E_{n} = E_{m}$ . These properties can be summarized using the Kronecker delta function

$$
\langle n | m \rangle = \delta_ {n, m}. \tag {23.16}
$$

# 23.2.1 Expansion of a General Wave Function

A very important theorem is that any wave function can be expanded in the set of all eigenfunctions,

$$
\psi (\vec {r}) = \sum_ {n} c _ {n} \psi_ {n} (\vec {r}). \tag {23.17}
$$

This can also be expressed more compactly using the ket notation

$$
| \psi \rangle = \sum_ {n} c _ {n} | n \rangle . \tag {23.18}
$$

The coefficients  $c_{n}$  are complex numbers that can be calculated from the wave functions,

$$
\langle m | \psi \rangle = \sum_ {n} c _ {n} \langle m | n \rangle = \sum_ {n} c _ {n} \delta_ {m, n} = c _ {m}. \tag {23.19}
$$

# 23.2.2 Magnitudes of Coefficients

Since wave functions are assumed to be normalized, there is a sum rule for the set of coefficients  $\{c_n\}$ ,

$$
1 = \langle \psi | \psi \rangle = \sum_ {n} \sum_ {m} c _ {n} ^ {*} c _ {m} \langle \psi_ {n} | \psi_ {m} \rangle = \sum_ {n} \sum_ {m} c _ {n} ^ {*} c _ {m} \delta_ {n, m} = \sum_ {n} | c _ {n} | ^ {2}. \tag {23.20}
$$

The expectation value of the energy can also be expressed in terms of the coefficients  $\{c_n\}$  and the energy eigenvalues

$$
\begin{array}{l} \langle \psi | H | \psi \rangle = \sum_ {m} \sum_ {n} c _ {m} ^ {*} c _ {n} \langle m | H | n \rangle \tag {23.21} \\ = \sum_ {m} \sum_ {n} c _ {m} ^ {*} c _ {n} \langle m | E _ {n} | n \rangle \\ = \sum_ {m} \sum_ {n} E _ {n} c _ {m} ^ {*} c _ {n} \delta m, n \\ = \sum_ {n} E _ {n} | c _ {n} | ^ {2}. \\ \end{array}
$$

Since the sum of  $|c_{n}|^{2}$  is 1, and the expectation value of the energy is given by the weighted sum in eq. (23.21), it is natural to interpret  $|c_{n}|^{2}$  as the probability that a measurement of the energy will put the system into the eigenstate  $|n\rangle$ .

It is important to note that if a system is in a general state,  $|\psi \rangle$ , given by eq. (23.18), it is not in an eigenstate unless  $|c_{n}| = 1$  for some value of  $n$ .  $|c_{n}|^{2}$  is the probability that the system would be found in the state  $|n\rangle$  only after a measurement has been made that put the system into an eigenstate. Without such a measurement, the probability that a general system is in an eigenstate is zero.

# 23.2.3 Phases of Coefficients

Since the time-dependence of the eigenfunctions is known, the time-dependence of an arbitrary wave function can be written in terms of the expansion in eigenfunctions,

$$
\psi (\vec {r}, t) = \sum_ {n} c _ {n} \exp \left(- i \frac {E _ {n}}{\hbar} t\right) \psi_ {n} (\vec {r}), \tag {23.22}
$$

or more compactly,

$$
| \psi , t \rangle = \sum_ {n} c _ {n} \exp \left(- i \frac {E _ {n}}{\hbar} t\right) | n \rangle . \tag {23.23}
$$

Eqs. (23.22) or (23.23) show that the time development of an arbitrary quantum state can be described by the changing phases of the expansion coefficients. They also show that eigenstates with different energies have phases that change at different rates, the relative phases between any two states, given by  $(E_{n} - E_{m})t / \hbar$ , sweep uniformly through all angles. This property suggests that a model probability distribution in equilibrium should be uniform in the phases. Indeed, if a probability distribution is not uniform in the phases, it will not be time-independent.

# 23.3 Many-Body Systems

Quantum systems that contain many particles ('many-body systems' in common terminology) are also described by a single wave function, but one that is a function of the coordinates of every particle in the system:  $\psi\left(\{\vec{r}_j | j = 1, \dots, N\}\right)$ . The Hamiltonian naturally depends on all the coordinates and all the gradients.

$$
H = H \left(\left\{\vec {r} _ {j}, - i \hbar \vec {\nabla} _ {j} | j = 1, \dots , N \right\}\right) \tag {23.24}
$$

We can again find eigenstates of the Hamiltonian (at least in principle), but the quantum 'number'  $n$  is now a set of  $3N$  numbers that are needed to describe the many-body wave function.

A general many-body wave function can also be expanded as a linear combination of eigenstates, as in eq. (23.18), and the time development of the many-body wave function can still be represented by eq. (23.23), with the appropriate interpretation of  $n$  as the set of many-body quantum numbers.

As in the single-particle case, the square of the absolute value of a coefficient in the expansion of a many-body wave function,  $|c_{n}|^{2}$ , can be interpreted as the probability that a measurement that put the system in an eigenstate would put it in eigenstate  $n$ . However, such a measurement is never carried out on a macroscopic system, which is never in an energy eigenstate.

# 23.4 Two Types of Probability

In Part I of this book we introduced the use of probabilities to describe our ignorance of the exact microscopic state of a many-body system. Switching from classical to quantum mechanics, we find a corresponding ignorance of the exact microscopic wave function. Again we need to construct a model probability distribution for the microscopic states to describe the behavior of a macroscopic system.

However, as we have seen previously, the properties of a quantum system are still only given by probability distributions even if we know the wave function. Therefore, in quantum statistical mechanics we need to deal with two kinds of probability distributions: one for the microscopic states, and a second for the observable properties.

# 23.4.1 Model Probabilities for Quantum Systems

We will denote a model probability distribution for the many-body wave functions as  $P_{\psi}$ . The calculation of the expectation value (average value) of any operator  $\mathcal{A}$  must include averages over probability distribution of the wave functions and the probability distribution of each quantum state,

$$
\langle \mathcal {A} \rangle = \int_ {\psi} P _ {\psi} \langle \psi | \mathcal {A} | \psi \rangle . \tag {23.25}
$$

We have written the average over  $P_{\psi}$  as an integral because there is a continuum of wave functions.

We can write eq. (23.25) in a more convenient form by expressing the wave function in terms of an expansion in eigenfunctions, as in eq. (23.18). Since the expansion coefficients completely specify the wave function, we can express the probability distribution in terms of the coefficients

$$
P _ {\psi} = P _ {\left\{c _ {n} \right\}}. \tag {23.26}
$$

The average of  $\mathcal{A}$  then becomes

$$
\langle \mathcal {A} \rangle = \int_ {\left\{c _ {n} \right\}} \sum_ {n} \sum_ {m} P _ {\left\{c _ {n} \right\}} c _ {m} ^ {*} c _ {n} \langle m | \mathcal {A} | n \rangle . \tag {23.27}
$$

The integral in eq. (23.27) is over all values of all coefficients, subject to the normalization condition in eq. (23.20).

Since the phases play an important role in specifying the time dependence in quantum mechanics, it will be useful to modify eq. (23.27) to exhibit them explicitly. If we write the coefficients as

$$
c _ {n} = \left| c _ {n} \right| \exp (i \phi_ {n}), \tag {23.28}
$$

we can rewrite eq. (23.27) as

$$
\langle \mathcal {A} \rangle = \int_ {\left\{\left| c _ {n} \right| \right\}} \int_ {\left\{\phi_ {n} \right\}} \sum_ {n} \sum_ {m} P _ {\left\{c _ {n} \right\}} \left| c _ {m} \right| \left| c _ {n} \right| \exp \left(- i \phi_ {m} + i \phi_ {n}\right) \langle m | \mathcal {A} | n \rangle . \tag {23.29}
$$

Since we have introduced the phases explicitly into eq. (23.29), we have also separated the integrals over the coefficients into integrals over their magnitudes and their phases.

# 23.4.2 Phase Symmetry in Equilibrium

Eq. (23.29) gives a formal expression for the macroscopic average of any property in any macroscopic system. However, since we are primarily interested in macroscopic equilibrium states, we can greatly simplify the problem by introducing the model probability for the phases mentioned in Subsection 23.2.3. Since the macroscopic equilibrium state is time-independent, it would seem reasonable to assume that the equilibrium probability distribution of phase angles should be uniform and the phases independent. With this assumption,  $P_{\{c_n\}} = P_{\{|c_n|\}}$ , and we can integrate over the phase angles when  $n \neq m$

$$
\int_ {0} ^ {2 \pi} d \phi_ {m} \int_ {0} ^ {2 \pi} d \phi_ {n} \exp (- i \phi_ {m} + i \phi_ {n}) = 0, \tag {23.30}
$$

or for  $n = m$

$$
\int_ {0} ^ {2 \pi} d \phi_ {n} \exp (- i \phi_ {n} + i \phi_ {n}) = \int_ {0} ^ {2 \pi} d \phi_ {n} = 2 \pi . \tag {23.31}
$$

Inserting eqs. (23.30) and (23.31) into eq. (23.29) we find

$$
\langle \mathcal {A} \rangle = 2 \pi \int_ {\left\{\left| c _ {n} \right| \right\}} \sum_ {n} P _ {\left\{\left| c _ {n} \right| \right\}} \left| c _ {n} \right| ^ {2} \langle n | \mathcal {A} | n \rangle , \tag {23.32}
$$

or

$$
\langle \mathcal {A} \rangle = \sum_ {n} \left[ 2 \pi \int_ {\{| c _ {n} | \}} P _ {\{| c _ {n} | \}} \left| c _ {n} \right| ^ {2} \right] \langle n | \mathcal {A} | n \rangle . \tag {23.33}
$$

Eq. (23.33) is particularly interesting. If we define a set of values  $\{P_n\}$ , where

$$
P _ {n} = 2 \pi \int_ {\left\{\left| c _ {n} \right| \right\}} P _ {\left\{\left| c _ {n} \right| \right\}} \left| c _ {n} \right| ^ {2}, \tag {23.34}
$$

we can rewrite the equation for  $\langle \mathcal{A}\rangle$  in a very useful form,

$$
\langle \mathcal {A} \rangle = \sum_ {n} P _ {n} \langle n | \mathcal {A} | n \rangle . \tag {23.35}
$$

The only quantum averages that appear in eq. (23.35) are of the form  $\langle n|\mathcal{A}|n\rangle$ , so every term in the sum is an average over a single quantum eigenstate. For the purposes of calculating the macroscopic properties of a macroscopic system in equilibrium, each eigenstate contributes separately.

The quantities  $P_{n}$  have two useful properties. First, since  $P_{\psi}$  is a probability distribution,  $P_{\psi} \geq 0$  for all  $\psi$ . From eq. (23.34), it follows that  $P_{n} \geq 0$  for all  $n$ . Next, since

the average of a constant must be the value of the constant, eq. (23.35) gives us a normalization condition on the set of quantities  $\{P_n\}$ ,

$$
1 = \langle 1 \rangle = \sum_ {n} P _ {n} \langle n | 1 | n \rangle = \sum_ {n} P _ {n}. \tag {23.36}
$$

This equation also implies that  $P_{n}\leq 1$

Since  $0 \leq P_{n} \leq 1$  for all  $n$ , and from eq. (23.36) the sum of all  $P_{n}$ 's is unity, the  $P_{n}$ 's fairly beg to be regarded as probabilities. The question is: what are they probabilities of?  $P_{n}$  can be interpreted as the probability that a measurement of the eigenstate of a system will result in the system being in eigenstate  $|n\rangle$ . However, it must be recognized that such a measurement is never made on a macroscopic system. Nevertheless, calculations using eq. (23.35) are carried out formally exactly as if the  $P_{n}$ 's were probabilities of something physically relevant. There is no real harm in referring to  $P_{n}$  as the 'probability of being in eigenstate  $|n\rangle$ ' (which is often found in textbooks), as long as you are aware of its true meaning. A more complete description of why this is a useful—if not entirely correct—way of thinking about quantum statistical mechanics is presented in Sections 23.5 and 23.6.

# 23.5 The Density Matrix

A very common representation of averages in a quantum ensemble is provided by the density operator, which is defined by a sum or integral over the microscopic states in an ensemble,

$$
\rho = \int_ {\psi} P _ {\psi} | \psi \rangle \langle \psi |. \tag {23.37}
$$

The density matrix  $\rho_{n,m}$  is found by taking the matrix element between the states  $\langle m|$  and  $|n\rangle$ ,

$$
\rho_ {m, n} = \int_ {\psi} P _ {\psi} \langle m | \psi \rangle \langle \psi | n \rangle . \tag {23.38}
$$

The density matrix is useful because it incorporates all the information needed to calculate ensemble averages. In particular, the ensemble average of an operator  $\mathcal{A}$  is given by

$$
\langle \mathcal {A} \rangle = T r [ \rho \mathcal {A} ] \tag {23.39}
$$

where  $Tr$  indicates the trace of the matrix. To prove eq. (23.39), simply evaluate the trace on the right-hand side of the equation,

$$
\begin{array}{l} T r [ \rho \mathcal {A} ] = \sum_ {n} \int_ {\psi} P _ {\psi} \langle n | \psi \rangle \langle \psi | \mathcal {A} | n \rangle \tag {23.40} \\ = \int_ {\psi} \sum_ {n} P _ {\psi} \langle n | \psi \rangle \langle \psi | \mathcal {A} | n \rangle \\ = \int_ {\{c _ {n} \}} \sum_ {n} \sum_ {\ell} \sum_ {m} P _ {\{c _ {n} \}} c _ {\ell} c _ {m} ^ {*} \langle n | \ell \rangle \langle m | \mathcal {A} | n \rangle \\ = \int_ {\{c _ {n} \}} \sum_ {n} \sum_ {m} P _ {\{c _ {n} \}} c _ {m} ^ {*} c _ {n} \langle m | \mathcal {A} | n \rangle = \langle \mathcal {A} \rangle . \\ \end{array}
$$

The last equality is found by comparison with eq. (23.27).

If we write the coefficients as  $c_{n} = |c_{n}|\exp (-i\phi_{n})$ , assume that the system is in equilibrium, and average over the phases, we find

$$
T r [ \rho \mathcal {A} ] = \int_ {\{c _ {n} \}} \sum_ {n} P _ {\{c _ {n} \}} | c _ {n} | ^ {2} \langle n | \mathcal {A} | n \rangle , \tag {23.41}
$$

or,

$$
T r [ \rho \mathcal {A} ] = \sum_ {n} P _ {n} \langle n | \mathcal {A} | n \rangle = \langle \mathcal {A} \rangle . \tag {23.42}
$$

This agrees with eq. (23.35).

# 23.6 Uniqueness of the Ensemble

A peculiar feature of the density matrix is that although eq. (23.40) shows that it contains the information to calculate any ensemble average, it does not uniquely specify the quantum ensemble. This can be seen from a simple example that compares the following two different quantum mechanical ensembles for a simple harmonic oscillator.

1. Ensemble 1: The SHO is either in the ground state  $|0\rangle$  with probability  $1/2$ , or it is in the first excited state  $|1\rangle$  with probability  $1/2$ . In either case, the system is in an eigenstate of the Hamiltonian. The density matrix operator corresponding to this ensemble is then

$$
\rho_ {1} = \frac {1}{2} (| 0 \rangle \langle 0 | + | 1 \rangle \langle 1 |). \tag {23.43}
$$

2. Ensemble 2: The system is in a state of the form

$$
| \phi \rangle = \frac {1}{\sqrt {2}} \left(| 0 \rangle + e ^ {i \phi} | 1 \rangle\right), \tag {23.44}
$$

with the values of  $\phi$  being uniformly distributed between 0 and  $2\pi$ . No member of the ensemble is an eigenstate of the Hamiltonian. The density matrix operator corresponding to this ensemble is then

$$
\begin{array}{l} \rho_ {2} = \frac {1}{2 \pi} \int_ {0} ^ {2 \pi} d \phi \frac {1}{\sqrt {2}} \left(| 0 \rangle + e ^ {i \phi} | 1 \rangle\right) \frac {1}{\sqrt {2}} \left(\langle 0 | + e ^ {- i \phi} \langle 1 |\right) \\ = \frac {1}{4 \pi} \int_ {0} ^ {2 \pi} d \phi \left(| 0 \rangle \langle 0 | + | 1 \rangle \langle 1 | + e ^ {- i \phi} | 0 \rangle \langle 1 | + e ^ {i \phi} | 1 \rangle \langle 0 |\right) \\ = \frac {1}{2} (| 0 \rangle \langle 0 | + | 1 \rangle \langle 1 |) = \rho_ {1}. \tag {23.45} \\ \end{array}
$$

This shows that the same density matrix operator describes both an ensemble that contains no eigenstates and an ensemble that contains only eigenstates. If we combine this with the result in eq. (23.40), we see that the expectation value of any operator is exactly the same for different ensembles as long as the density matrix operators are the same—even if the two ensembles do not have a single quantum state in common!

It is usual to express the density matrix operator in terms of the eigenstates of a system, which gives the impression that the ensemble also consists entirely of eigenstates. Even though we know that this is not the case, the previous demonstration shows that all predictions based on the (erroneous) assumption that a macroscopic system is in an eigenstate will be consistent with experiment.

# 23.7 The Planck Entropy

In the early days of quantum mechanics, Max Planck suggested defining the entropy of a quantum system as

$$
S _ {P} = k _ {B} \sum_ {\ell} \ln g _ {\ell}, \tag {23.46}
$$

where  $g_{\ell}$  is the number of eigenstates associated with energy level  $\ell$  and energy  $E_{\ell}$ , known as the degeneracy of the energy level. This form is analogous to the classical microcanonical entropy. Eq. (23.46) has a number of advantages, and is still quite often presented in textbooks as the correct definition of the entropy of a quantum system. It also has a number of disadvantages, starting with the discreteness of the set of energies for which it is defined.

It has been mainly applied to simple Hamiltonians, for which the degeneracies allow a straightforward approximation of the energy dependence of  $S_P$  by a continuum. For more complicated Hamiltonians, the energy levels can be split, reducing the degeneracy

of each level. If all the states become non-degenerate,  $g_{\ell} = 1$  for all  $\ell$ , and  $S_P = 0$ , which is clearly not correct.

In Chapter 24, we will compare the predictions of the Planck entropy with those of the canonical entropy.

# 23.8 The Quantum Microcanonical Ensemble

Now that we have reduced the problem of calculating averages in equilibrium quantum statistical mechanics to averages over individual eigenstates, we need only evaluate the set of 'probabilities',  $\{P_n\}$ , where  $n$  is the quantum number (or numbers) indexing the eigenstates. In principle this is straightforward, and we might try to follow the same procedure as in classical statistical mechanics. Consider a composite system, isolated from the rest of the universe, with fixed total energy  $E_{T} = E_{A} + E_{B}$ , assume all states are equally 'probable' (subject to the constraints), and calculate the probability distributions for observable quantities.

Unfortunately, we run into a technical difficulty that prevents us from carrying out this procedure for anything other than a particularly simple model.

If we assume that each subsystem is in an eigenstate—a nontrivial assumption that we return to in Chapter 24—the total energy of the composite system must be the sum of an energy eigenvalue from each subsystem,  $E = E_{n,A} + E_{m,B}$ . To calculate the probabilities for distributing the total energy among the two subsystems, we have to be able to take some amount of energy from subsystem  $A$  and transfer it to subsystem  $B$ . Unfortunately, the energy levels in a general macroscopic quantum system are not distributed in a regular fashion; if we change the energy in subsystem  $A$  to  $E_{n',A} = E_{n,A} + \Delta E$ , there will generally not be an eigenstate in subsystem  $B$  with an energy  $E_{m,B} - \Delta E$ . Even though the energy eigenvalues in a macroscopic system are very closely spaced, we will not always be able to transfer energy between the subsystem and maintain a constant total energy.

It is important to recognize that this is a problem that has arisen because we are assuming that each subsystem is in an eigenstate; all linear combinations of eigenstates, as in eq. (23.18), have been neglected.

Because macroscopic systems are never in an eigenstate, they do not have precisely specified energy. The typical spread in energy is far greater than the tiny gap between energy levels. Beyond that, the interactions between the two subsystems that are necessary to transfer energy between them will result in a single set of eigenvalues for the full composite system.

We must find a different way to do calculations. The simplest method is to make one of the subsystems extremely large and treat it as a thermal reservoir. As a quantum system becomes larger, its energy levels move closer together; in the limit of an infinite system, the energy spectrum becomes a continuum, and it is always possible to find a state with any desired energy. For this reason, we will abandon the microcanonical ensemble and turn to the canonical ensemble for most calculations in quantum statistical mechanics.