# Introduction

If, in some cataclysm, all scientific knowledge were to be destroyed, and only one sentence passed on to the next generation of creatures, what statement would contain the most information in the fewest words? I believe it is the atomic hypothesis (or atomic fact, or whatever you wish to call it) that all things are made of atoms—little particles that move around in perpetual motion, attracting each other when they are a little distance apart, but repelling upon being squeezed into one another. In that one sentence you will see an enormous amount of information about the world, if just a little imagination and thinking are applied.

Richard Feynman, in The Feynman Lectures on Physics

# 1.1 Thermal Physics

This book is about the things you encounter in everyday life: the book you are reading, the chair on which you are sitting, the air you are breathing. It is about things that can be hot or cold; hard or soft; solid, liquid, or gas. It is about machines that work for you: automobiles, heaters, refrigerators, air conditioners. It is even about your own body and the stuff of life. The whole subject is sometimes referred to as thermal physics, and it is usually divided into two main topics: thermodynamics and statistical mechanics.

Thermodynamics is the study of everything connected with heat. It provides powerful methods for connecting observable quantities by equations that are not at all obvious, but are nevertheless true for all materials. Statistical mechanics is the study of what happens when large numbers of particles interact. It provides a foundation for thermodynamics and the ultimate justification of why thermodynamics works. It goes beyond thermodynamics to reveal deeper connections between molecular behavior and material properties. It also provides a way to calculate the properties of specific objects, instead of just the universal relationships provided by thermodynamics.

The ideas and methods of thermal physics differ from those of other branches of physics. Thermodynamics and statistical mechanics each require their own particular ways of thinking. Studying thermal physics is not about memorizing formulas; it is about gaining a new understanding of the world.

# 1.2 What are the Questions?

Thermal physics seeks quantitative explanations for the properties of macroscopic objects, where the term 'macroscopic' means two things:

1. A macroscopic object is made up of a large number of particles.  
2. Macroscopic measurements have limited resolution.

The apparently vague terms 'large number' and 'limited resolution' take on specific meanings with respect to the law of large numbers that we will study in Chapter 3. We will see that the relative statistical uncertainty in quantities like the energy and the number of particles is usually inversely proportional to the square root of the number of particles. If this uncertainty is much smaller than the experimental resolution, it can be neglected. This leads to thermodynamics, which is a description of the macroscopic properties that ignores small statistical fluctuations.

We will assume that we somehow know what the object is made of; that is, what kinds of atoms and molecules it contains, and how many of each. We will generally speak of 'systems' instead of objects. The difference is that a system can consist of any number of objects, and is somehow separated from the rest of the universe. We will concentrate on systems in equilibrium; that is, systems that have been undisturbed long enough for their properties to take on constant values. We shall be more specific about what 'long enough' means in Chapter 22.

In the simplest case—which we will consider in Part I—a system might be completely isolated by 'adiabatic' walls; that is, rigid barriers that let nothing through. We will also assume, at least at first, that we know how much energy is contained in our isolated system.

Given this information, we will ask questions about the properties of the system. We will first study a simple model of a gas, so we will want to know what temperature and pressure are, and how they are related to each other and to the volume. We will want to know how the volume or the pressure will change if we heat the gas. As we investigate more complicated systems, we will see more complex behavior and find more questions to ask about the properties of matter.

As a hint of things to come, we will find that there is a function of the energy  $E$ , volume  $V$ , and number of particles  $N$  that is sufficient to answer all questions about the thermal properties of a system. It is called the entropy, and it is denoted by the letter  $S$ .

If we can calculate the entropy as a function of  $E, V$ , and  $N$ , we know everything about the macroscopic properties of the system. For this reason,  $S = S(E, V, N)$  is known as a fundamental relation, and it is the focus of Part I.

# 1.3 History

Atoms exist, combine into molecules, and form every object we encounter in life. Today, this statement is taken for granted. However, in the nineteenth century, and even into

the early twentieth century, these were fighting words. The Austrian physicist Ludwig Boltzmann (1844-1906) and a small group of other scientists championed the idea of atoms and molecules as the basis of a fundamental theory of the thermal properties of matter, against the violent objections of another Austrian physicist, Ernst Mach (1838-1916), and many of his colleagues. Boltzmann was right, of course, but the issue had not been settled even at the time of his tragic suicide in 1906. In the tradition of Boltzmann, the intention in this book is to present thermodynamics as a consequence of the molecular nature of matter.

The theory of thermodynamics was developed without benefit of the atomic hypothesis. It began with the seminal work of Sadi Carnot (French physicist, 1792-1832), who initiated the scientific discussion of heat engines. The First Law of Thermodynamics was discovered by James Prescott Joule (English physicist, 1818-1889), when he established that heat was a form of energy and measured the mechanical equivalent of heat. The Second Law of Thermodynamics was first stated in 1850 by Rudolf Clausius (German physicist, 1822-1888), who in 1865 also invented the related concept of entropy. The Second Law can be expressed by the statement that the entropy of an isolated system can increase, but not decrease.

Entropy was a mystery to nineteenth-century scientists. Clausius had given entropy an experimental definition that allowed it to be calculated, but its meaning was puzzling. Like energy, it could not be destroyed, but unlike energy, it could be created. It was essential to the calculation of the efficiency of heat engines (machines that turn the energy in hot objects into mechanical work), but it did not seem to be related to any other physical laws.

The reason why scientists working in the middle of the nineteenth century found entropy so mysterious is that few of them thought in terms of atoms or molecules. Even with molecular theory, explaining the entropy required brilliant insight; without molecular theory, there was no hope.

Serious progress in understanding the properties of matter and the origins of thermodynamic laws on the basis of the molecular nature of matter began in the 1860s with the work of Boltzmann and the American physicist J. Willard Gibbs (1839-1903).

Gibbs worked from a formal starting point, postulating that observable quantities could be calculated from an average over many replicas of an object in different microscopic configurations, and then working out what the equations would have to be. His work is very beautiful (to a theoretical physicist), although somewhat formal. However, it left certain questions unresolved—most notably, what has come to be called 'Gibbs' paradox'. This issue concerned a discontinuous change in the entropy when differences in the properties of particles in a mixture were imagined to disappear continuously. Gibbs himself did not regard this as a paradox. The issues involved are still a matter of debate in the twenty-first century.

Boltzmann devoted most of his career to establishing the molecular theory of matter and deriving the consequences of the existence of atoms and molecules. One of his great achievements was his 1877 definition of entropy—a definition which provides a physical interpretation of the entropy and the foundation of statistical mechanics. Unfortunately, although Boltzmann's paper is often cited, it is rarely read. This had led to frequent misinterpretations of what he meant. At least part of the problem

is that he wrote in a fairly dense nineteenth-century German, and an English translation has only appeared recently. We will discuss what he actually wrote in some detail.

Part I of this book is devoted to developing an intuitive understanding of the concept of entropy, based on Boltzmann's definition. We will present an explicit, detailed derivation of the entropy for a simple model to provide insight into its significance. Later parts of the book will develop more sophisticated tools for investigating thermodynamics and statistical mechanics, but they are all based on Boltzmann's definition of the entropy.

# 1.4 Basic Concepts and Assumptions

This book is concerned with the macroscopic behavior of systems containing many particles. This is a broad topic since everything we encounter in the world around us contains enormous numbers of atoms and molecules. Even in relatively small objects, there are  $10^{20}$  or more atoms.

Atoms and molecules are not the only examples of large numbers of particles in a system. Colloids, for example, can consist of  $10^{12}$  or more microscopic particles suspended in a liquid. The large number of particles in a typical colloid means that they are also well described by statistical mechanics.

Another aspect of macroscopic experiments is that they have limited resolution. We will see in Part I that the fluctuations in quantities like the density of a system are approximately inversely proportional to the square root of the number of particles in the system. If there are  $10^{20}$  molecules, this gives a relative uncertainty of about  $10^{-10}$  for the average density. Because it is rare to measure the density to better than one part in  $10^6$ , these tiny fluctuations are not seen in macroscopic measurements. Indeed, an important reason for Mach's objection to Boltzmann's molecular hypothesis was that no direct measurement of atoms had ever been carried out in the nineteenth century.

Besides the limited accuracy, it is rare for more than a million quantities to be measured in an experiment, and usually only a handful of measurements are made. Since it would take about  $6N$  quantities to specify the microscopic state of  $N$  atoms, thermodynamic measurements provide relatively little information about the microscopic state.

Due to our lack of detailed knowledge of the microscopic state of an object, we need to use probability theory—discussed in Chapters 3 and 5—to make further progress. However, we do not know the probabilities of the various microscopic states either. This means that we have to make assumptions about the probabilities. We will make

the simplest assumptions that are consistent with the physical constraints (number of particles, total energy, and so on): we will assume that everything we do not know is equally probable.

Based on our assumptions about the probability distribution, we will calculate the macroscopic properties of the system and compare our predictions with experimental data. We will find that our predictions are correct. This is comforting. However, we must realize that it does not necessarily mean that our assumptions were correct. In fact, we will see in Chapter 22 that many different assumptions would also lead to the same predictions. This is not a flaw in the theory but merely a fact of life. Recognizing this fact helps a great deal in resolving apparent paradoxes, as we will see.

The predictions we make based on assumptions about the probabilities of microscopic states lead to the postulates of thermodynamics. These postulates, in turn, are sufficient to derive the very powerful formalism of thermodynamics, as we will examine in Part II.

These same assumptions about the probabilities of microscopic states also lead to the even more powerful formalism of statistical mechanics, which we will investigate in the Parts III and IV of the book.

# 1.4.1 State Functions

It has long been known that when most macroscopic systems are left by themselves for a long period of time, their measurable properties stop changing and become time-independent. Simple systems, like a container of gas, evolve into macroscopic states that are well described by a small number of variables. For a simple gas, the energy, volume, and number of particles might be sufficient. These quantities, along with other quantities that we will discuss, are known as 'state functions'.

The molecules in a macroscopic system are in constant motion so that the microscopic state is constantly changing. This fact leads to a fundamental question. How is it that the macroscopic state can be time-independent with precisely defined properties? The answer to this question should become clear in Part I of this book.

# 1.4.2 Irreversibility

A second fundamental question is this. Even if there exist macroscopic equilibrium states that are independent of time, how can a system evolve toward such a state but not away from it? How can a system that obeys time-reversal-invariant laws of motion show irreversible behavior? This question has been the subject of much debate, at least since Johann Josef Loschmidt's (Austrian physicist, 1821-1895) formulation of the 'reversibility paradox' in 1876. We will present a resolution of the paradox in Chapter 22.

# 1.4.3 Entropy

The Second Law of Thermodynamics states that there exists a state function called the 'entropy' that is maximized in equilibrium. Boltzmann's 1877 definition of the entropy

provides an account of what this means. A major purpose of the calculation of the entropy of a classical ideal gas in Part I is to obtain an intuitive understanding of the significance of Boltzmann's entropy.

# 1.5 Plan of the Book

This book presents thermal physics as a consequence of the molecular nature of matter. The book is divided into four parts, in order to provide a systematic development of the ideas for someone coming to the subject for the first time, as well as for someone who knows the basic material but would like to review a particular topic.

Part I: Entropy The entropy will be introduced by an explicit calculation for the classical ideal gas. The ideal gas is a simple model with macroscopic properties that can be calculated exactly. This allows us to derive the entropy in closed form without any hidden assumptions. Since the entropy of the classical ideal gas exhibits most of the thermodynamic properties of more general systems, it will serve as an introduction to the otherwise rather abstract postulates of thermodynamics. In the last two chapters, Chapters 7 and 8 of Part I, the formal expression for the entropy of a classical gas with interacting particles is obtained, along with general expressions for the temperature, pressure, and chemical potential, which establish the foundations of classical statistical mechanics.

Part II: Thermodynamics The formal postulates of thermodynamics are introduced, based on the properties of the entropy derived in Part I. Our treatment follows the vision of László Tisz (Hungarian physicist, 1907-2009) and Herbert B. Callen (American physicist, 1919-1993). Although the original development of thermodynamics by nineteenth-century physicists was a brilliant achievement, their arguments were somewhat convoluted because they did not understand the microscopic molecular basis of the laws they had discovered. Deriving the equations of thermodynamics from postulates is much easier than following the historical path. The full power of thermodynamics can be developed in a straightforward manner and the structure of the theory made transparent.

Part III: Classical statistical mechanics Here we return to classical statistical mechanics to discuss more powerful methods of calculation. In particular, we introduce the canonical ensemble, which describes the behavior of a system in contact with a heat bath at constant temperature. The canonical ensemble provides a very powerful approach to solving most problems in classical statistical mechanics. We also introduce the grand canonical ensemble, which describes a system that can exchange particles with a large system at a fixed chemical potential. This ensemble will prove to be particularly important when we encounter it again in Chapters 27 through 30 of Part IV, where we discuss quantum systems of indistinguishable particles. Statistical mechanics can derive results that go beyond those of thermodynamics. We discuss and resolve the apparent

conflict between time-reversal-invariant microscopic equations with the obvious existence of irreversible behavior in the macroscopic world. Part III will introduce molecular dynamics and Monte Carlo computer simulations to demonstrate some of the modern methods for obtaining information about many-particle systems. We will also refine the calculation of the entropy to include the non-zero width of the energy and particle number distributions.

Part IV: Quantum statistical mechanics In the last part of the book, we develop quantum statistical mechanics, which is necessary for the understanding of the properties of most real systems. After two introductory chapters on basic ideas we will devote chapters to black-body radiation and lattice vibrations. There is a chapter on the general theory of indistinguishable particles, followed by individual chapters on the properties of bosons and fermions. Since the application of the theory of fermions to the properties of insulators and semiconductors has both theoretical and practical importance, this topic has a chapter of its own. The last chapter provides an introduction to the Ising model of ferromagnetism as an example of the theory of phase transitions.