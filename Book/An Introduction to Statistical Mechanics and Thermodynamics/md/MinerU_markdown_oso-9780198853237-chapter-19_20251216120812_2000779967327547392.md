# 19

# Ensembles in Classical Statistical Mechanics

In theory, there is no difference between theory and practice. In practice, there is.

Yogi Berra

This chapter begins Part III, in which we go more deeply into classical statistical mechanics. In Chapter 7, eq. (7.45) provided a formal expression for the entropy of a general classical system of interacting particles as a function of the energy, volume, and number of particles. In principle, this completes the subject of classical statistical mechanics, since it is an explicit formula that determines the fundamental thermodynamic relation for any classical system of interacting particles. In practice, it does not work that way.

Although eq. (7.45) is almost correct (we will have to modify it for first-order phase transitions in Chapter 21), carrying out a  $10^{20}$ -dimensional integral is non-trivial. It can be done for the ideal gas—which is why that example was chosen for Part I—but if the particles interact with each other, the whole business becomes much more difficult.

To make further progress we must develop more powerful formulations of the theory. The new expressions we derive will enable us to calculate some properties of interacting systems exactly. More importantly, they will enable us to make systematic approximations when we cannot find an exact solution.

The development of the new methods in classical statistical mechanics runs parallel to the development of the various representations of the fundamental relation in thermodynamics, which was discussed in Chapter 12. For every Legendre transform used in finding a new thermodynamic potential, there is a corresponding Laplace transform, named after Pierre-Simon, marquis de Laplace, French mathematician and scientist (1749-1827), that will take us to a new formulation of classical statistical mechanics. Just as different Legendre transforms are useful for different problems in thermodynamics, different Laplace transforms are useful for different problems in statistical mechanics.

With a slightly different interpretation, the canonical formalism will also allow us to refine the definition of the entropy. The idea is that we can use the canonical distribution as an improvement over the assumed delta-function distribution of the energy in defining

the Boltzmann entropy. This will prove especially useful when we discuss quantum mechanical models that exhibit negative temperatures.

In Part IV we will see that the correspondence between Legendre transforms in thermodynamics and ensembles in statistical mechanics carries over into quantum statistical mechanics.

# 19.1 Microcanonical Ensemble

In Chapter 7 we defined the classical microcanonical ensemble by a uniform probability distribution in phase space, subject to the constraints that the particles are all in a particular volume and that the total energy is constant. The constraint on the positions was realized mathematically by limiting the integrals over the positions to the volume containing the particles. The constraint on the energy was expressed by a Dirac delta function. The Hamiltonian (energy as a function of momentum and position) for a system of particles with pair-wise interactions is given by

$$
H (p, q) = \sum_ {j = 1} ^ {N} \frac {\left| \vec {p} _ {j} \right| ^ {2}}{2 m} + \sum_ {j = 1} ^ {N} \sum_ {i > j} ^ {N} \phi (\vec {r} _ {i}, \vec {r} _ {j}). \tag {19.1}
$$

The energy constraint is expressed by a delta function,

$$
\delta (E - H (p, q)). \tag {19.2}
$$

As in eq. (7.45), the entropy of a classical system of interacting particles is given by

$$
\begin{array}{l} S (E, V, N) = k \ln \Omega (E, V, N) \tag {19.3} \\ = k \ln \left[ \frac {1}{h ^ {3 N} N !} \int d q \int d p \delta (E - H (p, q)) \right]. \\ \end{array}
$$

The integrals  $\int dq\int dp$  in eq. (19.3) are over the  $6N$ -dimensional phase space, which, unfortunately, makes them difficult or impossible to carry out explicitly for interacting systems. In this chapter we will investigate several approaches to the problem that enable us to make accurate and efficient approximations for many cases of interest. The first approach, which we will consider in the next section, is the use of computer simulations.

# 19.2 Molecular Dynamics: Computer Simulations

Computer simulations are remarkably useful for providing us with a good intuitive feeling for the properties of ensembles in statistical mechanics, as well as evaluating measurable quantities. The most natural method of simulating the microcanonical

ensemble is known as Molecular Dynamics (MD). It consists of discretizing time, and then iterating discretized versions of Newton's equations of motion. Since Newton's equations conserve energy, MD is designed to explore a surface of constant energy in phase space, which is exactly what we want to do for the microcanonical ensemble.

For simplicity we will restrict the discussion to a single particle in one dimension. This avoids unnecessary notational complexity (indices) while demonstrating most of the important features of the method. The extension to more dimensions and more particles involves more programming, but no new ideas.

Consider a single particle in one dimension, moving under the influence of a potential energy  $V(x)$ . The equations of motion can be written in terms of the derivatives of the position and momentum

$$
\frac {d x}{d t} = \frac {p}{m} \tag {19.4}
$$

$$
\frac {d p}{d t} = - \frac {d V (x)}{d x}. \tag {19.5}
$$

The paths in phase space traced out by the solutions to these equations are called 'trajectories'.

The simplest way to discretize these equations is to define a time step  $\delta t$  and update the positions and momentum at every step.

$$
x (t + \delta t) = x (t) + \frac {p}{m} \delta t \tag {19.6}
$$

$$
p (t + \delta t) = p (t) - \frac {d V (x)}{d x} \delta t. \tag {19.7}
$$

While this is not the most efficient method, it is the easiest to program, and it is more than sufficient for the simulations in the assignments.

There are a quite a few ways of implementing discretized versions of Newton's equations—all of which go under the name of Molecular Dynamics. However, since our purpose is to understand physics rather than to write the most efficient computer program, we will only discuss the simplest method, which is given in eqs. (19.6) and (19.7).

There are two caveats:

1. Although solutions of the differential equations of motion (19.4) and (19.5) conserve energy exactly, the discretized equations (19.6) and (19.7) do not. As long as the time step  $\delta t$  is sufficiently short, the error is not significant. However, results should always be checked by changing the time step to determine whether it makes a difference.

2. Even if energy conservation is not a problem, it is not always true that an MD simulation will cover the entire surface of constant energy in phase space. If the trajectories do come arbitrarily close to any point on the constant energy surface, after a very long time, the system is said to be ergodic. Some examples in the assigned problems are ergodic, and some are not.

The MD simulations in the assignments will show, among other things, the distribution of positions in the microcanonical ensemble. The results will become particularly interesting when we compare them to the position distributions in the canonical ensemble (constant temperature, rather than constant energy), which we will explore with the Monte Carlo method in Section 19.11.

# 19.3 Canonical Ensemble

In this section we will show how to reduce the mathematical difficulties by calculating the properties of a system at fixed temperature rather than fixed energy. The result is known as the canonical ensemble, and it is an extremely useful reformulation of the essential calculations in statistical mechanics.

The canonical ensemble is the probability distribution in phase space for a system in contact with a thermal reservoir at a known temperature. The derivation of the canonical probability distribution is essentially the same as that of the Maxwell-Boltzmann probability distribution for the momentum of a particle, which was done in Subsection 8.3.1.

We will carry out the calculation in two different ways. First, we will calculate the energy distribution between the reservoir and the system of interest, based on the results summarized in Chapter 7. Next, we will calculate the probability density for the microstates (points in phase space) of the system of interest and show that the two calculations are consistent.

# 19.3.1 Canonical Distribution of the Energy

Assume that the system of interest is in thermal contact with a reservoir at temperature  $T$ . The entropy of the reservoir is given by an expression like that given in eq. (19.3),

$$
S _ {R} = k _ {B} \ln \Omega_ {R} \left(E _ {R}\right). \tag {19.8}
$$

We have suppressed the dependence on  $V_{R}$  and  $N_{R}$  in eq. (19.8) to make the notation more compact. Assume that the composite system of the thermal reservoir and the system of interest is isolated from the rest of the universe, so that the total energy of the composite system is fixed,

$$
E _ {T} = E + E _ {R}. \tag {19.9}
$$

As shown in Chapter 7, eq. (7.31), the probability distribution for the energy in the system of interest is given by

$$
P (E) = \frac {\Omega (E) \Omega_ {R} \left(E _ {T} - E\right)}{\Omega_ {T} \left(E _ {T}\right)}. \tag {19.10}
$$

Recall that the most important characteristic of a reservoir is that it is much bigger than the system of interest, so that  $E_{R} >> E$  and  $E_{T} >> E$ . We can use these inequalities to make an extremely good approximation to eq. (19.10).

Since  $\Omega_R$  is a rapidly varying function of energy, it is convenient to take the logarithm of both sides of eq. (19.10),

$$
\ln P (E) = \ln \Omega (E) + \ln \Omega_ {R} \left(E _ {T} - E\right) - \ln \Omega_ {T} \left(E _ {T}\right). \tag {19.11}
$$

Expand  $\ln \Omega_R$  in powers of  $E$ ,

$$
\begin{array}{l} \ln P (E) = \ln \Omega (E) + \ln \Omega_ {R} (E _ {T}) - E \left(\frac {\partial \ln \Omega_ {R} (E _ {T})}{\partial E _ {T}}\right) \\ - \ln \Omega_ {T} \left(E _ {T}\right) + \mathcal {O} \left(\left(E / E _ {T}\right) ^ {2}\right). \tag {19.12} \\ \end{array}
$$

Since the system and the reservoir are at the same temperature in equilibrium,

$$
T = T _ {R}, \tag {19.13}
$$

and

$$
\beta = \frac {1}{k _ {B} T} = \beta_ {R}, \tag {19.14}
$$

we have

$$
\frac {\partial \ln \Omega_ {R} \left(E _ {T}\right)}{\partial E _ {T}} = \beta_ {R} = \beta = \frac {1}{k _ {B} T}. \tag {19.15}
$$

Since  $\ln \Omega_R(E_T)$  and  $\ln \Omega_T(E_T)$  are constants, we can replace them in eq. (19.12) with a single constant that we will write as  $\ln Z$ . With this change of notation, insert eq. (19.15) in eq. (19.12),

$$
\ln P (E) = \ln \Omega (E) - \beta E - \ln Z. \tag {19.16}
$$

Exponentiating eq. (19.16), we find a general expression for the canonical probability distribution of the energy,

$$
P (E) = \frac {1}{Z} \Omega (E) \exp (- \beta E). \tag {19.17}
$$

In eq. (19.17),  $Z$  is simply a normalization 'constant'. It is constant in the sense that it does not depend on the energy  $E$ , although it does depend on  $T, V,$  and  $N$ , which are held constant during this calculation. Using the fact that  $P(E)$  must be normalized, an expression for  $Z$  is easy to obtain,

$$
Z (T, V, N) = \int \Omega (E, V, N) \exp (- \beta E) d E. \tag {19.18}
$$

The integral is over all possible values of the energy  $E$ .

The kind of integral transformation shown in eq. (19.18) is known as a Laplace transform. It is similar to a Fourier transform, but the factor in the exponent is real, rather than imaginary.

$Z$  turns out to be extremely important in statistical mechanics. It even has a special name: the 'partition function'. The reason for its importance will become evident in the rest of the book.

The universal use of the letter  $Z$  to denote the partition function derives from its German name, Zustandssumme: the sum over states. (German Zustand = English state.) Eq. (19.18) shows that  $Z$  is actually defined by an integral in classical mechanics, but in quantum mechanics the partition function can be written as a sum over eigenstates, as we will see in Chapter 23.

# 19.3.2 Canonical Distribution in Phase Space

This subsection derives the probability density of points in phase space for the canonical ensemble. This is more fundamental than the distribution of the energy since  $P(E)$  can be derived from the probability density in phase space by integrating over the delta function  $\delta (E - H(p,q))$ .

Denote the set of momentum and position variables describing the system of interest as  $\{p,q\}$  and the corresponding set for the reservoir as  $\{p_R,q_R\}$ . The usual assumption of a uniform probability distribution then applies to the total phase space described by all momentum and position variables from both systems, subject to conservation of the total energy,  $E_{T}$ .

To find the probability distribution in the phase space of the system of interest, we simply integrate out the variables  $\{p_R,q_R\}$  to obtain the marginal distribution  $P(p,q)$ . This gives us the canonical probability distribution at the temperature of the reservoir.

As shown in Chapter 7, eq. (7.31), the integral over  $\{p_R,q_R\}$  gives us  $\Omega_{R}$ . However, since the system of interest has an energy  $H(p,q)$ , the energy in the reservoir is  $E_{T} - H(p,q)$ . The resultant equation for the probability distribution in the phase space of the system of interest can be written as

$$
P (p, q) = \frac {\Omega_ {R} \left(E _ {T} - H (p , q)\right)}{\Omega_ {T} \left(E _ {T}\right)}. \tag {19.19}
$$

Since the reservoir is much larger than the system of interest,  $E_{T} > H(p,q)$ . Taking the logarithm of both sides of eq. (19.19) and expanding  $\ln \Omega_{R}$  in powers of  $H(p,q) / E_{T}$ , we find

$$
\ln P (p, q) = \ln \Omega_ {R} (E _ {T}) - H (p, q) \left(\frac {\partial (\ln \Omega_ {R} (E _ {T}))}{\partial E _ {T}}\right) - \ln \Omega_ {T} (E _ {T}) + \dots . \tag {19.20}
$$

Recalling that

$$
\beta = \beta_ {R} \equiv \frac {\partial (\ln \Omega_ {R} (E _ {T}))}{\partial E _ {T}} \tag {19.21}
$$

and noting that only the second term on the right in eq. (19.20) depends on  $p$  or  $q$ , we can write

$$
\ln P (p, q) = - \beta H (p, q) - \ln \tilde {Z} (T, V, N) \tag {19.22}
$$

or

$$
P (p, q) = \frac {1}{\tilde {Z} (T , V , N)} \exp [ - \beta H (p, q) ] \tag {19.23}
$$

The function  $\tilde{Z}(T, V, N)$  is given by the normalization condition

$$
\tilde {Z} (T, V, N) = \int d q \int d p \exp [ - \beta H (q, p) ]. \tag {19.24}
$$

The integral in eq. (19.24) is over all of phase space.

The only approximation in eq. (19.23) is the assumption that the reservoir is very large in comparison with the system of interest, which is usually an excellent approximation.

We still have to relate  $\tilde{Z}(T, V, N)$  to the partition function  $Z(T, V, N)$ , which we will do in the next section.

# 19.4 The Partition Function as an Integral over Phase Space

If we combine the general equation for  $\Omega$ , eq. (7.30),

$$
\Omega (E, V, N) = \frac {1}{h ^ {3 N} N !} \int d q \int d p \delta (E - H (q, p)), \tag {19.25}
$$

with the definition of the partition function in eq. (19.18),

$$
Z (T, V, N) = \int \Omega (E) \exp (- \beta E) d E, \tag {19.26}
$$

and carry out the integral over the delta function, we find a very useful expression for the partition function in terms of an integral over phase space,

$$
\begin{array}{l} Z = \int d E \frac {1}{h ^ {3 N} N !} \int d q \int d p \delta (E - H (q, p)) \exp (- \beta E) \\ = \frac {1}{h ^ {3 N} N !} \int d q \int d p \int d E \delta (E - H (q, p)) \exp (- \beta E) \\ = \frac {1}{h ^ {3 N} N !} \int d q \int d p \exp [ - \beta H (q, p) ]. \tag {19.27} \\ \end{array}
$$

The final expression in eq. (19.27) is probably the most important equation for practical calculations in classical statistical mechanics. You should be able to derive it, but it is so important that you should also memorize it.

Comparing eq. (19.27) to eq. (19.24), we see that  $\tilde{Z}(T, V, N)$  must be given by,

$$
\tilde {Z} (T, V, N) = h ^ {3 N} N! Z. \tag {19.28}
$$

The canonical probability density in phase space is then

$$
P (p, q) = \frac {1}{h ^ {3 N} N ! Z} \exp [ - \beta H (p, q) ]. \tag {19.29}
$$

# 19.5 The Liouville Theorem

Points in phase space move as functions of time to trace out trajectories. This rather obvious fact raises disturbing questions about the stability of the canonical distribution as a function of time. If a system is described by eq. (19.29) at a given time, will it still be described by the same distribution at a later time?

It is certainly not the case that an arbitrary, non-equilibrium probability distribution in phase space remains unchanged as time passes. We can see macroscopic changes in non-equilibrium systems, so the probability distribution must change.

Fortunately, for the canonical ensemble, the probability distribution is stable and does not change with time. This statement is based on the Liouville theorem, named after the French mathematician Joseph Liouville (1809-1882).

To prove the Liouville theorem, consider the probability density in phase space  $P(p,q)$ . Since points in phase space represent microscopic states, they can neither be created nor destroyed along a trajectory. We can then regard the points as abstract 'particles' moving in a 6N-dimensional space, with  $P(p,q)$  corresponding to the density of such points. As

for all gases of conserved particles, the points in phase space must obey a continuity equation,

$$
\frac {\partial P (p , q)}{\partial t} + \nabla \cdot (P (p, q) \vec {v}) = 0. \tag {19.30}
$$

The gradient in eq. (19.30) is defined by the vector,

$$
\nabla = \left\{\frac {\partial}{\partial q _ {j}}, \frac {\partial}{\partial p _ {j}} \right| j = 1, \dots , 3 N \}, \tag {19.31}
$$

and the  $6N$ -dimensional 'velocity' is defined by

$$
\vec {v} = \left\{\frac {\partial q _ {j}}{\partial t}, \frac {\partial p _ {j}}{\partial t} \mid j = 1, \dots , 3 N \right\} = \left\{\dot {q} _ {j}, \dot {p} _ {j} \mid j = 1, \dots , 3 N \right\}, \tag {19.32}
$$

where we have used a dot to indicate a partial derivative with respect to time.

The continuity equation then becomes

$$
\frac {\partial P}{\partial t} + \sum_ {j = 1} ^ {3 N} \left[ \frac {\partial}{\partial q _ {j}} (P (p, q) \dot {q} _ {j}) + \frac {\partial}{\partial p _ {j}} (P (p, q) \dot {p} _ {j}) \right] = 0 \tag {19.33}
$$

or

$$
\frac {\partial P}{\partial t} + P \sum_ {j = 1} ^ {3 N} \left[ \frac {\partial \dot {q} _ {j}}{\partial q _ {j}} + \frac {\partial \dot {p} _ {j}}{\partial p _ {j}} \right] + \sum_ {j = 1} ^ {3 N} \left[ \frac {\partial P}{\partial q _ {j}} \dot {q} _ {j} + \frac {\partial P}{\partial p _ {j}} \dot {p} _ {j} \right] = 0. \tag {19.34}
$$

Recall in classical mechanics that the time development of a point in phase space is given by Hamilton's equations, first derived by the Irish physicist and mathematician Sir William Rowan Hamilton (1805-1865)

$$
\dot {q} _ {j} = \frac {\partial H}{\partial p _ {j}} \tag {19.35}
$$

$$
\dot {p} _ {j} = - \frac {\partial H}{\partial q _ {j}}. \tag {19.36}
$$

$H = H(p,q)$  is, of course, the Hamiltonian. These two equations give us an interesting identity,

$$
\frac {\partial \dot {q} _ {j}}{\partial q _ {j}} = \frac {\partial^ {2} H}{\partial q _ {j} \partial p _ {j}} = \frac {\partial^ {2} H}{\partial p _ {j} \partial q _ {j}} = - \frac {\partial \dot {p} _ {j}}{\partial p _ {j}}. \tag {19.37}
$$

Inserting eq. (19.37) into eq. (19.34), we find that it simplifies to

$$
\frac {\partial P}{\partial t} + \sum_ {j = 1} ^ {3 N} \left[ \frac {\partial P}{\partial q _ {j}} \dot {q} _ {j} + \frac {\partial P}{\partial p _ {j}} \dot {p} _ {j} \right] = 0. \tag {19.38}
$$

To complete the proof of Liouville's theorem, we must only insert eq. (19.38) into the equation for the total derivative of  $P(p,q)$  with respect to time.

$$
\frac {d P}{d t} = \frac {\partial P}{\partial t} + \sum_ {j = 1} ^ {3 N} \left[ \frac {\partial P}{\partial q _ {j}} \dot {q} _ {j} + \frac {\partial P}{\partial p _ {j}} \dot {p} _ {j} \right] = 0 \tag {19.39}
$$

The interpretation of eq. (19.39) is that the probability density in the neighborhood of a moving point remains constant throughout the trajectory.

The application of Liouville's theorem to the canonical ensemble rests on the property that the canonical probability density depends only on the total energy. Since the trajectory of a point in phase space conserves energy, the canonical probability distribution does not change with time.

# 19.6 Consequences of the Canonical Distribution

The probability density,  $P(E)$ , for the energy in the canonical ensemble is extremely sharply peaked. The reason is that  $P(E)$  is the product of two functions that both vary extremely rapidly with energy:  $\Omega(E)$  increases rapidly with increasing  $E$ , while  $\exp(-\beta E)$  goes rapidly to zero.

To see that this is true, first recall that  $\Omega(E)$  for the ideal gas has an energy dependence of the form

$$
\Omega (E) \propto E ^ {f}, \tag {19.40}
$$

where  $f = 3N / 2$ . For more general systems,  $f$  is usually a relatively slowly varying function of the energy, but remains of the same order of magnitude as the number of particles  $N$ —at least to within a factor of 1000 or so—which is sufficient for our purposes.

If we approximate  $f$  by a constant, we can find the location of the maximum of  $P(E)$  by setting the derivative of its logarithm equal to zero,

$$
\frac {\partial \ln P (E)}{\partial E} = \frac {\partial}{\partial E} [ \ln \Omega (E) - \beta E - \ln Z ] = \frac {f}{E} - \beta = 0. \tag {19.41}
$$

The location of the maximum of  $P(E)$  gives the equilibrium value of the energy:

$$
E _ {e q} = \frac {f}{\beta} = f k _ {B} T. \tag {19.42}
$$

The width of the probability distribution for the energy can be found from the second derivative if we approximate  $P(E)$  by a Gaussian function, as discussed in connection with applications of Bayes' theorem in Section 5.5,

$$
\frac {\partial^ {2} \ln P (E)}{\partial E ^ {2}} = \frac {\partial}{\partial E} \left[ \frac {f}{E} - \beta \right] = - \frac {f}{E ^ {2}} = - \frac {1}{\sigma_ {E} ^ {2}}. \tag {19.43}
$$

Evaluating the expression for the second derivative in eq. (19.43) at  $E = E_{eq}$ , we find the width of  $P(E)$

$$
\sigma_ {E} = \frac {E _ {e q}}{\sqrt {f}}. \tag {19.44}
$$

When  $f$  is of the order  $N$ , and  $N \approx 10^{20}$  or larger, the relative width of the probability distribution is about  $10^{-10}$  or smaller. This is much smaller than the accuracy of thermodynamic measurements. Note that the  $1 / \sqrt{N}$  dependence is the same as we found for the width of the probability distribution for the number of particles in Chapter 4.

# 19.7 The Helmholtz Free Energy

The fact that the width of the probability distribution for the energy is roughly proportional to  $1 / \sqrt{N}$  in the canonical ensemble has an extremely important consequence, which is the basis for the importance of the partition function.

Consider eq. (19.16) for the logarithm of the canonical probability density for the energy

$$
\ln P (E) = - \beta E + \ln \Omega (E) - \ln Z. \tag {19.45}
$$

From eq. (7.45), we know that  $S = k_{B}\ln \Omega$ , so we can rewrite eq. (19.45) as

$$
\begin{array}{l} \ln Z = - \beta E + S / k _ {B} - \ln P (E) \tag {19.46} \\ = - \beta (E - T S) - \ln P (E) \\ = - \beta F - \ln P (E). \\ \end{array}
$$

Since the probability distribution  $P(E)$  is normalized and its width is roughly proportional to  $1 / \sqrt{N}$ , its height must be roughly proportional to  $\sqrt{N}$ . If we evaluate eq. (19.46) at the maximum of  $P(E)$ , which is located at  $E = E_{eq}$ , the term  $\ln P(E)$  must be of the order  $(1 / 2)\ln N$ . However, the energy  $E$ , the entropy  $S$ , and the free energy  $F = E - TS$  are all of order  $N$ . If  $N \approx 10^{20}$ , then

$$
\frac {\ln P \left(E _ {e q}\right)}{\beta F} \approx \frac {\ln N}{N} \approx 5 \times 1 0 ^ {- 1 9}. \tag {19.47}
$$

This means that the last term in eq. (19.46) is completely negligible. We are left with a simple relationship between the canonical partition function and the Helmholtz free energy

$$
\ln Z (T, V, N) = - \beta F (T, V, N) \tag {19.48}
$$

or

$$
F (T, V, N) = - k _ {B} T \ln Z (T, V, N). \tag {19.49}
$$

To remember this extremely important equation, it might help to recast the familiar equation,

$$
S = k _ {B} \ln \Omega , \tag {19.50}
$$

in the form

$$
\Omega = \exp \left[ S / k _ {B} \right]. \tag {19.51}
$$

In carrying out the Laplace transform in eq. (19.18), we multiplied by  $\Omega$  the factor  $\exp[-\beta E]$  before integrating over  $E$ . The integrand in eq. (19.18) can therefore be written as

$$
\begin{array}{l} \Omega \exp [ - \beta E ] = \exp [ - \beta E + S / k _ {B} ] \\ = \exp [ - \beta (E - T S) ] \\ = \exp [ - \beta F ]. \tag {19.52} \\ \end{array}
$$

# 19.8 Thermodynamic Identities

Since the postulates of thermodynamics are based on the results of statistical mechanics, it should not be too surprising that we can derive thermodynamic identities directly from statistical mechanics.

For example, if we take the partial derivative of the logarithm of the partition function in eq. (19.27) with respect to  $\beta$ , we find that it is related to the average energy

$$
\begin{array}{l} \frac {\partial}{\partial \beta} \ln Z = \frac {1}{Z} \frac {\partial Z}{\partial \beta} \\ = \frac {1}{Z} \frac {\partial}{\partial \beta} \left[ \frac {1}{h ^ {3 N} N !} \int d q \int d p \exp [ - \beta H (q, p) ] \right] \\ = - \frac {1}{Z} \frac {1}{h ^ {3 N} N !} \int d q \int d p H (q, p) \exp [ - \beta H (q, p) ] \\ = - \frac {\int d q \int d p H (q , p) \exp [ - \beta H (q , p) ]}{\int d q \int d p \exp [ - \beta H (q , p) ]} \\ = - \langle E \rangle . \tag {19.53} \\ \end{array}
$$

Since we know that  $\ln Z = -\beta F$ , eq. (19.53) is equivalent to the thermodynamic identity

$$
U = \langle E \rangle = \frac {\partial (\beta F)}{\partial \beta}, \tag {19.54}
$$

which was proved as an exercise in Chapter 14.

# 19.9 Beyond Thermodynamic Identities

Because statistical mechanics is based on the microscopic structure of matter, rather than the phenomenology that led to thermodynamics, it is capable of deriving relationships that are inaccessible to thermodynamics. An important example is a general relationship between the fluctuations of the energy and the specific heat.

Begin with the expression for the average energy from eq. (19.53),

$$
\begin{array}{l} U = \langle E \rangle \\ = \frac {1}{Z} \frac {1}{h ^ {3 N} N !} \int d q \int d p H (q, p) \exp [ - \beta H (q, p) ] \\ = \frac {\int d q \int d p H (q , p) \exp [ - \beta H (q , p) ]}{\int d q \int d p \exp [ - \beta H (q , p) ]}. \tag {19.55} \\ \end{array}
$$

Note that the dependence on the temperature appears in only two places in eq. (19.55): once in the numerator and once in the denominator. In both cases, it appears in an exponent as the inverse temperature  $\beta = 1 / k_{B}T$ . It is usually easier to work with  $\beta$  than with  $T$  in statistical mechanical calculations. To convert derivatives, either use the identity,

$$
\frac {d \beta}{d T} = \frac {d}{d T} \left(\frac {1}{k _ {B} T}\right) = \frac {- 1}{k _ {B} T ^ {2}}, \tag {19.56}
$$

or the identity

$$
\frac {d T}{d \beta} = \frac {d}{d \beta} \left(\frac {1}{k _ {B} \beta}\right) = \frac {- 1}{k _ {B} \beta^ {2}}. \tag {19.57}
$$

The specific heat per particle at constant volume can then be written as

$$
c _ {V} = \frac {1}{N} \left(\frac {\partial U}{\partial T}\right) _ {V, N} = \frac {1}{N} \frac {\partial}{\partial T} \langle E \rangle = \frac {1}{N} \frac {- 1}{k _ {B} T ^ {2}} \frac {\partial}{\partial \beta} \langle E \rangle . \tag {19.58}
$$

We can now take the partial derivative of  $\langle E\rangle$  in eq. (19.55),

$$
\begin{array}{l} \frac {\partial}{\partial \beta} \langle E \rangle = \frac {\partial}{\partial \beta} \left[ \frac {\int d q \int d p H (q , p) \exp [ - \beta H (q , p) ]}{\int d q \int d p \exp [ - \beta H (q , p) ]} \right] \\ = \frac {\int d q \int d p H (q , p) (- H (q , p)) \exp [ - \beta H (q , p) ]}{\int d q \int d p \exp [ - \beta H (q , p) ]} \\ - \frac {\int d q \int d p H (q , p) \exp [ - \beta H (q , p) ] \int d q \int d p (- H (q , p)) \exp [ - \beta H (q , p) ]}{\left[ \int d q \int d p \exp [ - \beta H (q , p) ] \right] ^ {2}} \\ = - \langle E ^ {2} \rangle + \langle E \rangle^ {2}, \tag {19.59} \\ \end{array}
$$

and use eq. (19.58) to find the specific heat:

$$
c _ {V} = \frac {1}{N} \frac {- 1}{k _ {B} T ^ {2}} \frac {\partial}{\partial \beta} \langle E \rangle = \frac {1}{N k _ {B} T ^ {2}} \left(\langle E ^ {2} \rangle - \langle E \rangle^ {2}\right). \tag {19.60}
$$

In relating the fluctuations in the energy to the specific heat, eq. (19.60) goes beyond thermodynamics. It gives us a direct connection between the microscopic fluctuations and macroscopic behavior. This relationship turns out to be extremely useful in computational statistical mechanics, where it has been found to be the most accurate method for calculating the specific heat from a computer simulation.

It should be clear from the derivation of eq. (19.60) that it can be generalized to apply to the derivative of any quantity of interest with respect to any parameter in the Hamiltonian. The generality and flexibility of this technique has found many applications in computational statistical mechanics.

# 19.10 Integration over the Momenta

A great advantage of working with the canonical partition function is that the integrals over the momenta can be carried out exactly for any system in which the forces do not depend on the momenta. This, unfortunately, eliminates systems with moving particles in magnetic fields, but still leaves us with an important simplification for most systems of interest. As an example, we will consider the case of pairwise interactions for simplicity, although the result is still valid for many-particle interactions.

If  $H(p,q)$  only depends on the momenta through the kinetic energy, the partition function in eq. (19.27) becomes

$$
\begin{array}{l} Z = \frac {1}{h ^ {3 N} N !} \int d p \int d q \exp \left[ - \beta \left(\sum_ {j = 1} ^ {N} \frac {| \vec {p} _ {j} | ^ {2}}{2 m} + \sum_ {j = 1} ^ {N} \sum_ {i > j} ^ {N} \phi (\vec {r} _ {i}, \vec {r} _ {j})\right) \right] \\ = \frac {1}{h ^ {3 N} N !} \int d p \exp \left[ - \beta \sum_ {k = 1} ^ {3 N} \frac {\vec {p} _ {k} ^ {2}}{2 m} \right] \int d q \exp \left[ - \beta \sum_ {j = 1} ^ {N} \sum_ {i > j} ^ {N} \phi (\vec {r} _ {i}, \vec {r} _ {j}) \right] \\ = \frac {1}{h ^ {3 N} N !} (2 \pi m k _ {B} T) ^ {3 N / 2} \int d q \exp \left[ - \beta \sum_ {j = 1} ^ {N} \sum_ {i > j} ^ {N} \phi (\vec {r} _ {i}, \vec {r} _ {j}) \right], \tag {19.61} \\ \end{array}
$$

where we have used  $\beta = 1 / k_{B}T$  in the last line of eq. (19.61). The momenta have been integrated out exactly.

# 19.10.1 The Classical Ideal Gas

For the ideal gas, the interactions between particles vanish  $[\phi (\vec{r}_i,\vec{r}_j) = 0]$ , and we can rederive the main results of Part I in a few lines. The integrals remaining in eq. (19.61) give a factor of  $V$  for each particle, so that,

$$
\begin{array}{l} Z = \frac {1}{h ^ {3 N} N !} (2 \pi m k _ {B} T) ^ {3 N / 2} \int d q \exp (0) \\ = \frac {1}{h ^ {3 N} N !} (2 \pi m k _ {B} T) ^ {3 N / 2} V ^ {N}. \tag {19.62} \\ \end{array}
$$

The logarithm of eq. (19.62) gives the Helmholtz free energy,  $F = -k_{B}T\ln Z$ , of the classical ideal gas as a function of temperature, volume, and number of particles, which is a representation of the fundamental relation and contains all thermodynamic information. It is left as an exercise to confirm that the explicit expression for  $F$  agrees with that derived from the entropy of the classical ideal gas in Part I.

# 19.10.2 Computer Simulation in Configuration Space

Eq. (19.61) is used extensively in Monte Carlo computer simulations because it eliminates the need to simulate the momenta and therefore reduces the required computational effort. Monte Carlo computations of the thermal properties of models in statistical mechanics will be discussed in the next section.

# 19.11 Monte Carlo Computer Simulations

Just as the Molecular Dynamics method is the most natural way to simulate a microcanonical ensemble (constant energy), the Monte Carlo (MC) method is the most natural way to simulate a canonical ensemble (constant temperature). The MC method ignores the natural trajectories of the system in favor of a random sampling that

reproduces the canonical probability distribution. In practice, applications of MC to classical statistical mechanics usually simulate the equilibrium canonical probability distribution for the configurations after the momenta have been integrated out,

$$
P _ {e q} (q) = \frac {1}{Q} \exp [ - \beta V (q) ]. \tag {19.63}
$$

In eq. (19.63), which follows directly from eq. (19.29) upon integrating out the momenta, the normalization constant  $Q$  plays a role very similar to the partition function. It is defined by

$$
Q \equiv \int \exp [ - \beta V (q) ] d q, \tag {19.64}
$$

where the integral goes over the allowed volume in configuration space.

To implement an MC simulation, we first construct a stochastic process for probability distributions defined in configuration space. A stochastic process is a random sequence of states (configurations in our case) in which the probability of the next state depends on the previous states and the parameters of the model (temperature, interaction energies, and so on). Our goal is to find a stochastic process for which an arbitrary initial probability distribution will evolve into the desired equilibrium distribution

$$
\lim  _ {t \rightarrow \infty} P (q, t) = P _ {e q} (q) = \frac {1}{Q} \exp [ - \beta V (q) ]. \tag {19.65}
$$

Once we have such a process we can use it to generate states with the equilibrium probability distribution and average over those states to calculate thermal properties.

If the next state in a stochastic process does not depend on states visited before the previous one, it is called a Markov process. Since we will see that they are particularly convenient for simulations, we will consider only Markov processes for MC simulations.

For a given Markov process, we can define a conditional probability for a particle to be in configuration  $q'$  at time  $t + \delta t$  if it was in configuration  $q$  at time  $t$ . We will denote this conditional probability as  $W(q'|q)$ . (It is also called a transition probability and is often denoted by  $W(q \to q')$  to indicate the direction of the transition.) Since the transition from one state to the next in a Monte Carlo simulation is discrete,  $\delta t$  is called the time step and is not an infinitesimal quantity.

The change in the probability density  $P(q, t)$  for  $q$  between times  $t$  and  $t + \delta t$  is then given by the 'Master Equation',

$$
P (q, t + \delta t) - P (q, t) = \int \left[ W \left(q \mid q ^ {\prime}\right) P \left(q ^ {\prime}, t\right) - W \left(q ^ {\prime} \mid q\right) P (q, t) \right] d q ^ {\prime}, \tag {19.66}
$$

where the integral in eq. (19.66) goes over all allowed configurations  $q'$ . The term  $W(q|q')P(q',t)$  in the integrand of eq. (19.66) represents the increase in the probability

of state  $q$  due to transitions from other states, while  $W(q'|q)P(q,t)$  represents the decrease in the probability of state  $q$  due to transitions out of that state. Obviously, the transition probabilities must be normalized, so that,

$$
\int W \left(q ^ {\prime} | q\right) d q ^ {\prime} = 1, \tag {19.67}
$$

for all  $q$

A necessary condition for the simulation to go to equilibrium as indicated in eq. (19.65) is that once the probability distribution reaches  $P_{eq}(q)$ , it stays there. In equilibrium,  $P(q,t) = P_{eq}(q)$  is independent of time, and the master equation becomes

$$
P _ {e q} (q) - P _ {e q} (q) = 0 = \int \left[ W (q | q ^ {\prime}) P _ {e q} (q ^ {\prime}) - W (q ^ {\prime} | q) P _ {e q} (q) \right] d q ^ {\prime}. \tag {19.68}
$$

Although the Markov process will remain in equilibrium as long as the integral in eq. (19.68) vanishes, the more stringent condition that the integrand vanishes turns out to be more useful,

$$
W \left(q \mid q ^ {\prime}\right) P _ {e q} \left(q ^ {\prime}\right) - W \left(q ^ {\prime} \mid q\right) P _ {e q} (q) = 0. \tag {19.69}
$$

Eq. (19.69) is called the condition of 'detailed balance', because it specifies that the number of transitions between any two configurations is the same in both directions.

It can be shown that if there is a finite sequence of transitions such that the system can go from any configuration to any other configuration with non-zero probability and that detailed balance holds, the probability distribution will go to equilibrium as defined in eq. (19.65). After the probability distribution has gone to equilibrium, we can continue the Markov process and use it to sample from  $P_{eq}(q)$ .

The condition that there is a finite sequence of transitions such that the system can go from any configuration to any other configuration with non-zero probability is called 'ergodicity'. This terminology is unfortunate, because the word 'ergodicity' is also used to refer to the property that dynamical trajectories come arbitrarily close to any given point on an energy surface, as discussed in connection with MD simulations. It is much too late in the history of the subject to produce words that clearly distinguish these two concepts, but we may take some consolation in the fact that one meaning is used only with MC, and the other only with MD.

The great thing about the condition of detailed balance in eq. (19.69) is that we are able to decide what the conditional probabilities  $W(q'|q)$  will be. For simulating a canonical ensemble, we want to use this power to create a Markov process that produces the canonical distribution,

$$
\lim  _ {t \rightarrow \infty} P (q, t) = P _ {e q} (q) = \frac {1}{Q} \exp [ - \beta V (q) ]. \tag {19.70}
$$

As long as the conditional (transition) probabilities do not vanish, the condition of detailed balance can be written as,

$$
\frac {W \left(q ^ {\prime} \mid q\right)}{W \left(q \mid q ^ {\prime}\right)} = \frac {P _ {e q} \left(q ^ {\prime}\right)}{P _ {e q} (q)} = \exp \left[ - \beta \left(V \left(q ^ {\prime}\right) - V (q)\right) \right], \tag {19.71}
$$

or,

$$
\frac {W \left(q ^ {\prime} \mid q\right)}{W \left(q \mid q ^ {\prime}\right)} = \exp [ - \beta \Delta E ], \tag {19.72}
$$

where  $\Delta E = V(q') - V(q)$ . Note that the 'partition function',  $Q$ , has canceled out of eq. (19.72)—which is very convenient, since we usually cannot evaluate it explicitly. There are many ways of choosing the  $W(q'|q)$  to satisfy eq. (19.72). The oldest and simplest is known as the Metropolis algorithm.

Metropolis algorithm:

A trial configuration  $q_{\text{trial}}$  is chosen at random.

- If  $\Delta E \leq 0$ , the new state is  $q' = q_{trial}$ .  
- If  $\Delta E > 0$ , the new state is  $q' = q_{\text{trial}}$  with probability  $\exp(-\beta \Delta E)$  and the same as the old state  $q' = q$  with probability  $1 - \exp(-\beta \Delta E)$ .

It is easy to confirm that the Metropolis algorithm satisfies detailed balance.

There are many things that have been swept under the rug in this introduction to the Monte Carlo method. Perhaps the most serious is that there is nothing in eq. (19.70) that says that convergence will occur within your computer budget. Slow MC processes can also make acquiring data inefficient, which has prompted a great deal of effort to go into devising more efficient algorithms. There are also many methods for increasing the amount of information acquired from MC (and MD) computer simulations. Fortunately, there are a number of excellent books on computer simulation methods available for further study.

We will again restrict our examples in the assignments to a single particle in one dimension, as we did in Section 19.2 for Molecular Dynamics simulations. Remember to compare the results of the two simulation methods to see the different kinds of information each reveals about the thermal properties of the system.

# 19.12 Factorization of the Partition Function: The Best Trick in Statistical Mechanics

The key feature of the Hamiltonian that allowed us to carry out the integrals over the momenta in eq. (19.61) and the integrals over the coordinates in eq. (19.62) is that the integrand  $\exp [-\beta H]$  can be written as a product of independent terms. Writing a  $3N$ -dimensional integral as a one-dimensional integral raised to the  $3N$ -th power takes a nearly impossible task and makes it easy.

We can extend this trick to any system in which the particles do not interact with each other, even when they do interact with a potential imposed from outside the system. Consider the following Hamiltonian:

$$
H (p, q) = \sum_ {j = 1} ^ {N} \frac {\left| \vec {p} _ {j} \right| ^ {2}}{2 m} + \sum_ {j = 1} ^ {N} \tilde {\phi} _ {j} (\vec {r} _ {j}). \tag {19.73}
$$

The partition function for the Hamiltonian in eq. (19.73) separates neatly into a product of three-dimensional integrals:

$$
\begin{array}{l} Z = \frac {1}{h ^ {3 N} N !} (2 \pi m k _ {B} T) ^ {3 N / 2} \int d q \exp \left[ - \beta \sum_ {j = 1} ^ {N} \tilde {\phi} _ {j} (\vec {r} _ {j}) \right] \\ = \frac {1}{h ^ {3 N} N !} (2 \pi m k _ {B} T) ^ {3 N / 2} \int d q \prod_ {j = 1} ^ {N} \exp \left[ - \beta \tilde {\phi_ {j}} (\vec {r _ {j}}) \right] \\ = \frac {1}{h ^ {3 N} N !} (2 \pi m k _ {B} T) ^ {3 N / 2} \prod_ {j = 1} ^ {N} \int d \vec {r} _ {j} \exp \left[ - \beta \tilde {\phi} _ {j} (\vec {r} _ {j}) \right]. \tag {19.74} \\ \end{array}
$$

Eq. (19.74) has reduced the problem from one involving an integral over a  $3N$ -dimensional space, as in eq. (19.61), to a product of  $N$  three-dimensional integrals.

When the three-dimensional integrals are all the same, eq. (19.74) simplifies further:

$$
Z = \frac {1}{h ^ {3 N} N !} (2 \pi m k _ {B} T) ^ {3 N / 2} \left(\int d \vec {r} _ {1} \exp \left[ - \beta \tilde {\phi} _ {1} (\vec {r} _ {1}) \right]\right) ^ {N}. \tag {19.75}
$$

Since a three-dimensional integral can always be evaluated, even if only numerically, all problems of this form can be regarded as solved.

This trick is used repeatedly in statistical mechanics. Even when it is not possible to make an exact factorization of the partition function, it is often possible to make a good approximation that does allow factorization.

Factorization of the partition function is the most important trick you need for success in statistical mechanics. Do not forget it!

# 19.13 Simple Harmonic Oscillator

The simple harmonic oscillator plays an enormous role in statistical mechanics. The reason might be that many physical systems behave as simple harmonic oscillators, or it might be just that it is one of the few problems we can solve exactly. It does provide a nice example of the use of factorization in the evaluation of the partition function.

# 19.13.1 A Single Simple Harmonic Oscillator

First consider a single, one-dimensional, simple harmonic oscillator (SHO). The Hamiltonian is given by the following equation, where the subscript 1 indicates that there is just a single SHO:

$$
H _ {1} = \frac {1}{2} K x ^ {2} + \frac {p ^ {2}}{2 m}. \tag {19.76}
$$

The partition function is found by simplifying eq. (19.27). The integrals over  $x$  and  $p$  are both Gaussian, and can be carried out immediately,

$$
\begin{array}{l} Z _ {1} = \frac {1}{h} \int_ {- \infty} ^ {\infty} d x \int_ {- \infty} ^ {\infty} d p \exp \left(- \beta \left(\frac {1}{2} K x ^ {2} + \frac {1}{2 m} p ^ {2}\right)\right) \\ = \frac {1}{h} (2 \pi k _ {B} T / K) ^ {1 / 2} (2 \pi m k _ {B} T) ^ {1 / 2} \\ = \frac {1}{\beta \hbar} \left(\frac {m}{K}\right) ^ {1 / 2} \\ = \frac {1}{\beta \hbar \omega} \tag {19.77} \\ \end{array}
$$

where

$$
\omega = \sqrt {\frac {K}{m}} \tag {19.78}
$$

is the angular frequency of the SHO.

As we have mentioned earlier, the factor of  $\hbar$  has no true significance within classical mechanics. However, we will see in Section 24.10 that the result in eq. (19.77) does coincide with the classical limit of the quantum simple harmonic oscillator given in eq. (24.71).

# 19.13.2 N Simple Harmonic Oscillators

Now consider a macroscopic system consisting of  $N$  one-dimensional simple harmonic oscillators,

$$
H _ {N} = \sum_ {j = 1} ^ {N} \left(\frac {1}{2} K _ {j} x _ {j} ^ {2} + \frac {p _ {j} ^ {2}}{2 m _ {j}}\right). \tag {19.79}
$$

Note that the spring constants  $K_{j}$  and the masses  $m_{j}$  might all be different.

Since the oscillating particles are localized by the harmonic potential, we do not need to consider exchanges of the particles with another system. Therefore, we do not include an extra factor of  $1 / N!$  to appear in the partition function. Actually, since  $N$  is never varied in this model, a factor of  $1 / N!$  would not have any physical consequences. However, excluding it produces extensive thermodynamic potentials that are esthetically pleasing.

After integrating out the momenta, the partition function can be written as,

$$
\begin{array}{l} Z _ {N} = \frac {1}{h ^ {N}} \prod_ {j = 1} ^ {N} \left(2 \pi m _ {j} k _ {B} T\right) ^ {1 / 2} \int_ {- \infty} ^ {\infty} d ^ {N} x \exp \left(- \beta \sum_ {j = 1} ^ {N} \frac {1}{2} K _ {j} x _ {j} ^ {2}\right) \\ = \frac {1}{h ^ {N}} \prod_ {j = 1} ^ {N} \left[ \left(2 \pi m _ {j} k _ {B} T\right) ^ {1 / 2} \int_ {- \infty} ^ {\infty} d x _ {j} \exp \left(- \beta \frac {1}{2} K _ {j} x _ {j} ^ {2}\right) \right] \\ = \frac {1}{h ^ {N}} \prod_ {j = 1} ^ {N} \left[ \left(2 \pi m _ {j} k _ {B} T\right) ^ {1 / 2} \left(2 \pi k _ {B} T / K _ {j}\right) ^ {1 / 2} \right] \\ = \left(\frac {2 \pi k _ {B} T}{h}\right) ^ {N} \prod_ {j = 1} ^ {N} \left(\frac {m _ {j}}{K _ {j}}\right) ^ {1 / 2} \\ = \prod_ {j = 1} ^ {N} \left(\frac {1}{\beta \hbar \omega_ {j}}\right), \tag {19.80} \\ \end{array}
$$

where

$$
\omega_ {j} = \sqrt {\frac {K _ {j}}{m _ {j}}} \tag {19.81}
$$

is the angular frequency of the  $j$ -th SHO.

If all  $N$  SHO's have the same frequency, the partition function simplifies further,

$$
Z _ {N} = \left(\frac {1}{\beta \hbar \omega_ {1}}\right) ^ {N}. \tag {19.82}
$$

# 19.14 Problems

# PROBLEM 19.1

# Two one-dimensional relativistic particles

Consider two one-dimensional relativistic ideal-gas particles with masses confined to a one-dimensional box of length  $L$ . Because they are relativistic, their energies are given by  $E_{A} = |p_{A}|c$  and  $E_{A} = |p_{B}|c$ .

Assume that the particles are in thermal equilibrium with each other, and that the total kinetic energy is  $E = E_{A} + E_{B}$ . Use the usual assumption that the probability density is uniform in phase space, subject to the constraints.

Calculate the probability distribution  $P(E_A)$  for the energy of one of the particles.

# PROBLEM 19.2

# Classical simple harmonic oscillator

Consider a one-dimensional, classical, simple harmonic oscillator with mass  $m$  and potential energy

$$
\frac {1}{2} K x ^ {2}.
$$

The SHO is in contact with a thermal reservoir (heat bath) at temperature  $T$ .

Calculate the classical partition function, Helmholtz free energy, and energy.

# PROBLEM 19.3

# Canonical ensemble for anharmonic oscillators

We have shown that the classical canonical partition function is given by

$$
Z = \frac {1}{h ^ {3 N} N !} \int d p ^ {3 N} \int d q ^ {3 N} \exp \left[ - \beta H (p, q) \right].
$$

Consider  $N$  non-interacting (ideal gas) particles in an anharmonic potential,

$$
\frac {K}{r} \left(| x | ^ {r} + | y | ^ {r} + | z | ^ {r}\right)
$$

where  $r$  is a positive constant.

The full Hamiltonian can be written as:

$$
H (p, q) = \sum_ {n = 1} ^ {N} \frac {p ^ {2}}{2 m} + \frac {K}{r} \sum_ {j = 1} ^ {3 N} | q _ {j} | ^ {r}.
$$

Calculate the energy and the specific heat using the canonical partition function.

Hint: The integrals over the momenta shouldn't cause any problems, since they are the same as for the ideal gas. However, the integrals over the positions require a trick that we used in deriving the value of the gaussian integral.

Another hint: You might find that there is an integral that you can't do, but don't need.

# PROBLEM 19.4

# Partition function of ideal gas

1. Calculate the partition function and the Helmholtz free energy of the classical ideal gas using the canonical ensemble.  
2. Starting with the entropy of the classical ideal gas, calculate the Helmholtz free energy and compare it with the result from the canonical ensemble.

# PROBLEM 19.5

# Molecular Dynamics (MD) simulation of a simple harmonic oscillator

This assignment uses the Molecular Dynamics method, which simply consists of discretizing Newton's equations of motion, and then iterating them many times to calculate the trajectory.

Write a computer program to perform a Molecular Dynamics (MD) computer simulation of a one-dimensional simple harmonic oscillator (SHO),

$$
H = \frac {1}{2} K x ^ {2}.
$$

[You can take  $K = 1$  in answering the questions.]

Include a plot of the potential energy, including a horizontal line indicating the initial energy. [You will also do Monte Carlo (MC) simulations of the same models later, so remember to make your plots easy to compare.]

The program should read in the starting position, the size of the time step, and the number of iterations. It should then print out a histogram of the positions visited during the simulation.

Your program should also print out the initial and final energy as a check on the accuracy of the algorithm. Don't forget to include the kinetic energy in your calculation!

1. For a starting value of  $x = 1.0$ . Try various values of the size of the time step to see the effect on the results.

What happens when the time step is very small or very large?

2. Choose  $x = 1.0$  and run the program for an appropriate value of  $dt$ . Print out histograms of the positions visited. [Later, you will compare them with you results for MC.]

# PROBLEM 19.6

# Molecular Dynamics (MD) simulation of an anharmonic oscillator

This assignment again uses the Molecular Dynamics method. Only minor modifications of your code should be necessary, although I would recommend using functions to simplify the programming.

Write a computer program to perform a Molecular Dynamics (MD) computer simulation of a one-dimensional particle in the following potential:

$$
H = A x ^ {2} + B x ^ {3} + C x ^ {4}.
$$

[The values of  $A, B,$  and  $C$  should be read in or set at the beginning of the program.]

Include a plot of the potential energy, including a horizontal line indicating the initial energy.

The program should read in the starting position, the size of the time step, and the number of iterations. It should then print out a histogram of the positions visited during the simulation.

Your program should also print out the initial and final energy as a check on the accuracy of the algorithm. Don't forget to include the kinetic energy in your calculation!

1. Carry out simulations with the values  $A = -1.0$ ,  $B = 0.0$ , and  $C = 1.0$ .

First try various starting values of the position  $x$ . Choose the starting positions to adjust the (constant) total energy to take on 'interesting' values. Part of the problem is to decide what might be interesting on the basis of the plot of the potential energy.

2. Now change the value of  $B$  to  $-1.0$ , while retaining  $A = -1.0$  and  $C = 1.0$ . Again look for interesting regions.

# PROBLEM 19.7

Molecular Dynamics (MD) simulation of a chain of simple harmonic oscillators

This assignment again uses the Molecular Dynamics method, but this time we will simulate a chain of particles connected by harmonic interactions.

Write a computer program to perform a Molecular Dynamics (MD) computer simulation of the following Hamiltonian:

$$
H = \sum_ {j = 1} ^ {N} \frac {p _ {j} ^ {2}}{2 m} + \frac {1}{2} K \sum_ {j = 1} ^ {N} \left(x _ {j} - x _ {j - 1}\right) ^ {2}.
$$

Take the particle at  $x_0 = 0$  to be fixed.

1. For various (small) values of  $N$ , try different values of the time step while monitoring the change in total energy during the simulation. See if a good choice of time step agrees with what you found for a single particle.  
2. For  $N = 1$ , make sure that you get the same answer as you did for HW2. [Just confirm that you did it; you don't have to hand it in.]  
3. For  $N = 2$  and 3, and random starting values of position and momentum, what distributions do you get for  $x_{1}$  and  $x_{N - 1} - x_{N - 2}$ ?  
4. For  $N = 10$ , start particle 1 with an initial position of  $x[1] = 1.0$ , let the initial positions of all other particles be 0.0, and let the initial value of all momenta be 0.0. What distribution do you get for  $x_{j} - x_{j-1}$  for whatever value of  $j$  you choose?  
5. Which results resemble what you might expect if your chain were in equilibrium at some temperature? Can you estimate what that temperature might be?

# PROBLEM 19.8

# Molecular Dynamics (MD) simulation of a chain of simple harmonic oscillators - AGAIN

This assignment again uses the MD program you wrote to simulate a chain of particles connected by harmonic interactions:

$$
H = \sum_ {j = 1} ^ {N} \frac {p _ {j} ^ {2}}{2 m} + \frac {1}{2} K \sum_ {j = 1} ^ {N} \left(x _ {j} - x _ {j - 1}\right) ^ {2}.
$$

[The particle at  $x_0 = 0$  is fixed.]

1. You know that the average energy of a single simple harmonic oscillator at temperature  $T$  is given by

$$
U = \langle H \rangle = k _ {B} T,
$$

so that the specific heat is

$$
c = k _ {B}.
$$

Demonstrate analytically that the energy per particle and the specific heat per particle are the same for a harmonic chain and a single SHO without using a Fourier transform of the Hamiltonian.

2. Modify your program to calculate the expected kinetic energy per particle from the initial (randomly chosen) energy. Then use the average kinetic energy to predict an effective temperature.  
3. Modify your program to find the contributions to the specific heat from the fluctuations of the potential energy under the assumption that the kinetic degrees of freedom act as a thermal reservoir at the predicted effective temperature.

[The contributions to the specific heat from the momenta are  $k_{B} / 2$  per particle.]

4. Calculate the effective temperature and the specific heat for various lengths of the harmonic chain. To what extent are the relationships between the energy and the effective temperature and the fluctuations and the specific heat confirmed by your data?

# PROBLEM 19.9

# Not-so-simple harmonic oscillators

Consider a one-dimensional, generalized oscillator with Hamiltonian

$$
H = K | x | ^ {\alpha}
$$

with the parameter  $\alpha > 0$ .

Calculate the energy and specific heat as functions of the temperature for arbitrary values of  $\alpha$ .

# PROBLEM 19.10

# Monte Carlo simulations of not-so-simple harmonic oscillators

Consider a one-dimensional, generalized oscillator with Hamiltonian

$$
H = A | x | ^ {\alpha}.
$$

1. Modify your program for MC simulations to simulate the potential energy given in this problem. For values of  $\alpha = 0.5, 1.0$ , abd 4.0, run MC simulations and compare the results to your theoretical results for the heat capacity from an earlier assignment.

# PROBLEM 19.11

# Relativistic particles in a gravitational field

Consider  $N$  relativistic ideal-gas particles at temperature  $T$ . The particles are confined to a three-dimensional box of area  $A$  and  $0 < z < \infty$ . Because they are relativistic, the Hamiltonian is given by

$$
H (p, q) = \sum_ {j = 1} ^ {N} | \vec {p} _ {j} | c + \sum_ {j = 1} ^ {N} m g z _ {j}.
$$

1. Find the probability distribution for the height of a single particle, i.e.:  $P(z_{1})$ .  
2. Find the average height of a single particle.  
3. In what way did your answer for the average height depend on the relativistic nature of the particles?  
4. Find the probability distribution for the energy of a single particle,  $E_{1} = |\vec{p}_{1}|c + mgz_{1}$ , including the normalization constant.

# PROBLEM 19.12

# A classical model of a rubber band

The rubber band is modeled as a one-dimensional polymer, that is, a chain of  $N$  monomers. Each monomer has a fixed length  $\delta$ , and the monomers are connected end-to-end to form the polymer. Each monomer is completely free to rotate in two dimensions (there are no steric hindrances from the other monomers). A microscopic state of the rubber band is therefore described by the set of angles

$$
\theta \equiv \{\theta_ {n} | n = 1, 2, \ldots , N \}
$$

describing the orientations of the monomers. [You can decide whether to define the angles with respect to the orientation of the previous monomer, or according to a fixed frame of reference. However, you must say which you've chosen!]

The rubber band (polymer) is used to suspend a mass  $M$  above the floor. One end of the rubber band is attached to the ceiling, and the other end is attached to the mass, so that the mass is hanging above the floor by the rubber band. For simplicity, assume

that the rubber band is weightless. You can ignore the mass of the monomers and their kinetic energy.

The whole system is in thermal equilibrium at temperature  $T$ .

1. Calculate the energy of the system (polymer plus mass) for an arbitrary microscopic state  $\theta$  of the monomers (ignoring the kinetic energy).  
2. Find an expression for the canonical partition function in terms of a single, one-dimensional integral.  
3. Find an expression for the average energy  $U$ .  
4. For very high temperatures, find the leading term in an expansion of the average energy  $U$ .