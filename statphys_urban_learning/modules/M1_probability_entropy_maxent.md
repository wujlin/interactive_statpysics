# M1 概率·计数·熵·最大熵（MaxEnt）

> **核心目标**：本章是整个现代统计物理的逻辑起点。我们要学会“**在信息不足时如何做最合理的推断**”。你将掌握从线性约束直接写出指数族分布（Boltzmann/Gibbs）的能力，这将是你处理后续所有复杂系统的通用武器。

## 推荐学习顺序
1. 通读 Introduction 与 Part 1–4，理解 Jaynes 的“推断视角”。
2. 手推一遍 `exercises/written/E01`（从 MaxEnt 导出 Boltzmann）。
3. 跑通 `exercises/notebooks/E01`，看看拉格朗日乘子是如何被数值解出来的。

## 背景最小包（First Principles）
- 我们不是在猜“真实分布”，而是在信息不足时做**最诚实的推断**。
- 计数比动力学更稳：在宏观约束下，“可能的微观态有多少”往往比“系统怎么走”更关键。
- 只要约束是线性的，最大熵的解就是指数族（这会贯穿后续所有系综）。

## 逻辑桥接（M0 → M1）
- M0 给了你宏观记账语法（势函数与偏导），但还没回答“微观态概率从何而来”。
- M1 用熵与最大熵把“宏观约束”翻译成“微观分布”，这是统计物理真正起步的地方。

## 阅读材料（带读）：Swendsen Ch5 + Jaynes 1957

下面我按“带读”的方式把 **Swendsen Ch5（5.1–5.6）** 和 **Jaynes 1957（只读 Abstract + Introduction）** 串成一条主线：**信息缺失 → 概率描述 →（在约束下）最大熵选分布**。（Swendsen 章节文本见原书）

---

### 0｜你现在只需要盯住一个核心问题

> **当我不知道系统的“精确微观状态”时，我还能做出哪些“最不瞎猜”的描述？**  
> Swendsen 给你“概率论工具箱”；Jaynes 给你“怎么选概率分布才不夹带私货”的原则。

---

### Part A｜Swendsen Ch5（5.1–5.6）逐节带读

#### 5.1 连续骰子：为什么“点概率=0”，但“区间概率≠0”

**你读这一节要得到的直觉：**  
连续变量里，“取到某个精确点”的概率必然是 0（因为可能性无限多），但“落在一个区间里”的概率可以是非零，用**积分**算。

- 关键式子：区间概率与区间长度成正比  
\[
P([a,b])=\int_a^b P(x)\,dx
\]
- “公平连续骰子”例子：\(P(x)=1/6\)，所以 \(P([a,b])=(b-a)/6\)。

**停一下自测（1 句话）**

- 为什么连续情形不能说“区间里点更多，所以更可能”？（提示：每个区间“点数”都是同一个无限大）

---

#### 5.2 概率密度：密度有单位、可以大于 1 ——但积分必须等于 1

**你读这一节要抓住两件事：**

1. **概率是无量纲；概率密度有量纲**。  
   例如在体积 \(V\) 的盒子里均匀分布：\(P(x,y,z)=1/V\)，单位是 \(m^{-3}\)。

2. **密度可以 > 1，甚至某点发散也可能合法**，只要全空间积分归一：
\[
\int_\Omega P(x)\,dx=1
\]

然后这节顺手把“离散概率论”搬到连续里（你以后会反复用）：

- **边缘化（marginal）**：把你“不关心/看不见”的变量积分掉  
\[
P_x(x)=\int P(x,y)\,dy
\]
- **条件概率**：已知 \(x\) 的信息后，\(y\) 的分布怎么变  
\[
P(y|x)=\frac{P(x,y)}{P_x(x)}
\]
- **独立性**：\(P(x,y)=P_x(x)P_y(y)\)（等价于“知道 \(y\) 对 \(x\) 没信息增益”）

**停一下自测**

- 你能用“信息缺失”解释边缘化吗？（答：因为我不知道/不观测到 \(y\)，只能把所有可能的 \(y\) 汇总进去）

---

#### 5.3 Dirac \(\delta\)：它不是“概率”，它是“约束/筛选器”

这一节是统计物理里极其常用的“算分布神器”。

**你只要记住 \(\delta\) 的两句话：**

1. \(\delta\) 用来“把积分的贡献锁定在满足约束的位置”
2. 它会带来一个“雅可比因子” \(\frac{1}{|g'(x_i)|}\)

核心性质（“抽样/取值”）：
\[
\int f(x)\,\delta(x-x_0)\,dx=f(x_0)
\]
推广到一般约束 \(g(x)=0\)（有多个根就求和）：
\[
\int f(x)\delta(g(x))\,dx=\sum_i \frac{f(x_i)}{\left|g'(x_i)\right|}
\]

**你可以把 \(\delta\) 当成：**“把**连续的不确定性**压到**满足条件的那条曲线/那个点**上。”

---

#### 5.4 变量变换：用 \(\delta\) 一步写出新变量的分布

你以后做“从轨迹到速度/能耗/到达时间分布”的时候，本质就是这一节。

Swendsen 给的通用公式非常重要：若 \(s=f(x,y)\)，则
\[
P(s)=\iint P(x,y)\,\delta\!\left(s-f(x,y)\right)\,dx\,dy
\]
它的意思是：**所有会产生同一个 \(s\) 的 \((x,y)\) 都要贡献概率**，\(\delta\) 负责“挑出这些 \((x,y)\)”。

他用“两个均匀连续骰子求和”做例子，结果是经典的“三角形分布”：

- \(s\le 6\)：越接近 6，组合越多，所以密度线性上升
- \(s\ge 6\)：越接近 12，组合越少，所以线性下降

---

#### 5.5 Bayes：把“缺信息”变成“可量化的后验不确定性”

这一节直接把 Bayes 放到“实验反推参数”的语境里：
\[
P(\theta|X)=\frac{P(X|\theta)P(\theta)}{P(X)}
\]

- 先验 \(P(\theta)\)：实验前你知道多少
- 似然 \(P(X|\theta)\)：如果参数是 \(\theta\)，会看到数据 \(X\) 的概率
- 后验 \(P(\theta|X)\)：看完数据后的合理信念

他还做了一个非常实用的结论：当样本数 \(N\) 很大，后验会很尖，**不确定性尺度 \(\sim 1/\sqrt{N}\)**（你在任何统计推断里都会见到这个缩放）。

---

#### 5.6 Problems：你不必全做，但建议用来“验收理解”

如果只挑两题来“验收你真的读懂”：

- **5.1(1)**：归一化 + 尾概率（练积分直觉）
- **5.6(1)**：Maxwell-Boltzmann 一维高斯归一化（为后续玻尔兹曼/MB 分布铺路）

#### 5.6 Problems：两道题的核心讲解（归一化直觉）

这两个题其实都在练同一件事：**“概率密度 \(P(x)\) 不是概率本身，真正的概率要对区间/区域积分；而且总概率必须归一化为 1。”**

---

##### 题 5.1(1)：归一化 + 尾概率（tail probability）

题目给：
$$
P(x)=A e^{-2x},\quad x>0
$$

**(a) “归一化常数 \(A\)”是什么意思？**

意思是：**所有可能的 \(x\) 加起来（积分起来）概率必须是 1**：
$$
\int_0^\infty P(x)\,dx=1
\Rightarrow
\int_0^\infty A e^{-2x}\,dx=1
$$

算：
$$
\int_0^\infty e^{-2x}\,dx=\left[-\frac{1}{2}e^{-2x}\right]_0^\infty=\frac{1}{2}
$$
所以
$$
A\cdot \frac{1}{2}=1 \Rightarrow A=2
$$

**(b) “尾概率 \(P(x>1)\)”怎么理解？**

尾概率就是**落在阈值右侧那段“尾巴”面积**：
$$
P(x>1)=\int_1^\infty P(x)\,dx=\int_1^\infty 2e^{-2x}\,dx
$$
$$
=2\left[-\frac{1}{2}e^{-2x}\right]_1^\infty=e^{-2}
$$

> 直觉：这是指数分布的“右尾”，阈值越大，尾巴面积指数衰减。

---

##### 题 5.6(1)：一维 Maxwell–Boltzmann（高斯）归一化

题目给：
$$
P(v_x)=A\exp\!\left(-\beta \frac{1}{2}mv_x^2\right)
$$
要求 \(A\)。

**(a) 这题在做什么？**

同样是归一化：速度分量 \(v_x\) 可以取任意实数（正负都有），所以
$$
\int_{-\infty}^{\infty} P(v_x)\,dv_x = 1
$$

令
$$
a=\frac{\beta m}{2} \quad(>0)
$$
则
$$
1=\int_{-\infty}^{\infty} A e^{-a v_x^2} dv_x
= A \int_{-\infty}^{\infty} e^{-a v^2} dv
$$

**(b) 关键积分（高斯积分）**

你需要用（或记住）：
$$
\int_{-\infty}^{\infty} e^{-a v^2} dv = \sqrt{\frac{\pi}{a}}
$$
于是
$$
1=A\sqrt{\frac{\pi}{a}}
\Rightarrow
A=\sqrt{\frac{a}{\pi}}
=\sqrt{\frac{\beta m}{2\pi}}
$$

**答案：**
$$
\boxed{A=\sqrt{\frac{\beta m}{2\pi}}}
$$

> 物理直觉：\(T\) 越大（\(\beta\) 越小），分布越“宽”；\(T\) 越小（\(\beta\) 越大），分布越“窄”，速度更集中在 0 附近。

---

**补一句：这两个题与“\(1/Z = e^{\lambda-1}\)”的关系**

这两个题里的 \(A\) 本质上就是 **\(1/Z\)**，因为
$$
Z=\int e^{-\text{(指数项)}}\,d(\text{变量}),\quad A=\frac{1}{Z}
$$
归一化就是在“算配分函数”。

---

### Part B｜Jaynes 1957：只读 Abstract + Introduction（带读要点）

#### B1 Abstract：一句话翻译成你的“研究语言”

Jaynes 在摘要里非常明确地说：

- 我们只有**部分知识（partial knowledge）**，因此需要构造概率分布
- 最大熵（maximum-entropy estimate）是“在已知信息下最不偏、对缺失信息最不武断”的选择
- 统计力学如果被看作推断，那么很多规则（从配分函数开始）可由最大熵直接推出

> 你要带走的关键词：**least biased / maximally noncommittal / missing information**。

#### B2 Introduction：他到底想解决什么“不舒服的地方”

引言开头的语气是：统计力学发展很多年，但“从微观力学推到宏观热力学”的论证一直不够令人满意；历史上为补洞引入了额外假设（例如某些遍历性/等先验想法），这让基础不够干净。

然后他提出一个关键转向（你读引言时就抓这个）：

- **同一个形式** \(-\sum p_i\log p_i\) 同时出现在信息论和统计力学，并不自动意味着它们“本质相同”
- 需要一个新视角：把统计力学当成**在约束下的推断问题**；你知道的约束很少（宏观量），微观概率未知，于是要用“最大熵”来选分布

---

### Part C｜把两份阅读焊在一起：一条最短主线

1. 连续变量里，我们几乎从来不知道精确值，只能谈区间/密度（Swendsen 5.1–5.2）。
2. 对看不见的变量要边缘化；对新信息要条件化/Bayes 更新（Swendsen 5.2, 5.5）。
3. 当你只知道少数宏观约束时，仍有“巨大缺信息”：那么**哪一个分布最不瞎猜？**
4. Jaynes：选最大熵分布——对缺失信息最不武断，于是玻尔兹曼形式/配分函数等规则自然出现。

---

### 你现在的“验收作业”（写出来就算完成阅读任务）

请你用 4 句话回答（每句不超过 20 字）：

1. 连续变量为什么点概率为 0？
2. 概率密度为什么可以大于 1？
3. 边缘化在信息层面是什么意思？
4. 最大熵在避免什么？

你把这 4 句发我，我就能立刻判断你哪里已经通了、哪里还卡着，然后我们再进入下一章（通常就是从 MaxEnt 推出玻尔兹曼/MB 分布）。

#### 校正版要点（先做再看）

1) **点概率为 0**：单点集合测度为 0，概率是对集合的积分，  
   \(P(X=x_0)=\int_{\{x_0\}} p(x)\,dx=0\)；只有区间概率才有意义。  
2) **密度可大于 1**：\(p(x)\) 是“单位 \(x\)”上的概率，带量纲；概率要靠积分，  
   \(\int p(x)\,dx=1\)。密度值可大于 1，只要可积且归一。  
3) **边缘化**：对未观测/不关心变量求和或积分，汇总所有未知分支，  
   \(p(x)=\int p(x,y)\,dy\)。  
4) **最大熵**：在已知约束成立的前提下，避免对未知部分引入额外结构/偏见（最不武断）。  

#### 精炼版（可直接放进笔记，约 100 字）

连续变量中单点集测度为 0，故点概率为 0；区间概率由密度积分给出。概率密度是“每单位 \(x\)”上的概率，带量纲，积分归一，值可大于 1。边缘化是对未观测变量求和/积分，把未知分支汇总到 \(p(x)\)。最大熵是在已知约束下避免额外结构与偏见的最保守选择。

---

## Introduction

在 M0 中我们建立了热力学势的语言。但当我们把这套宏观语言应用到微观粒子（或城市个体）时，一个巨大的**历史困境**出现了：
微观力学方程（牛顿方程或薛定谔方程）在时间上是完全可逆的。如果把电影倒放，原路返回的粒子完全遵守物理定律。
**但是**，宏观世界却充满了不可逆：墨水滴入水中会扩散，但绝不会自发聚拢；城市早高峰的拥堵一旦形成，也绝不会自发消散。

这个“微观可逆 vs 宏观不可逆”的矛盾（Loschmidt Paradox），曾让 Boltzmann 备受攻击。他的反击不仅挽救了热力学，更开创了一种全新的世界观：**我们观测到的“定律”，其实只是“大概率事件”的集合。**

## References
- Jaynes 1957：导读见 [Seminal papers](/references/seminal_papers)（条目：`SP-M1-Jaynes1957-I`；想深入可再读 `SP-M1-Jaynes1957-II`）。
- Boltzmann 1877：导读见 [Seminal papers](/references/seminal_papers)（条目：`SP-M1-Boltzmann1877`）。

---

## Part 1：上帝在扔骰子吗？不，他在计数

Boltzmann 的洞见是：宏观不可逆并不需要一条“趋向平衡的力”，它来自计数上的压倒性优势。给定同样的宏观约束，平衡态对应的微观实现方式数（多重度 \(\Omega\)）通常远大于有序态；系统的随机演化因此几乎总把你带向 \(\Omega\) 更大的那一类宏观态（可对照 Context：[[Boltzmann 不可逆性与概率解释（历史语境）]]）。

### Interactive：熵与计数（可选）
不要只相信直觉，请亲自试一试。
下方有两个盒子（左L/右R）。初始时所有 50 个球都在左边（高度有序）。
点击 **Start Mixing**，系统会随机挑选一个球移动到另一边。请观察：
1. 球会自发地变得左右均匀吗？
2. 一旦均匀（~25/25）后，它还能自发回到全在左边的状态吗？

<InteractiveConcept type="entropy-counter" />

### 关键观察
你刚才看到的不可逆过程，并没有依靠近任何“趋向平衡的力”。甚至可以说，根本没有“热力学第二定律”这条硬性规则，有的只是**概率的碾压**。
- 全左状态（$N_L=50, N_R=0$）：$\Omega = 1$ 种方式。
- 均匀状态（$N_L=25, N_R=25$）：$\Omega = \frac{50!}{25!25!} \approx 1.26 \times 10^{14}$ 种方式。

这就是为什么熵 $S = k \ln \Omega$ 总是增加：**系统只是在不知不觉中流向了那种“有更多种活法”的状态。**

如果要把这段直觉写成可推导的公式，核心对象是宏观态对应的多重度 \(\Omega\) 及其对数 \(S=k\ln\Omega\)。相关定义与计算工具见：[[微观态与宏观态 Microstate vs macrostate]]、[[多重度 multiplicity Ω]]、[[熵 Entropy]]、[[Stirling 近似 Stirling approximation]]。

---

## Part 2：Jaynes 的选择——在无知中保持诚实

现在问题来了：如果系统不仅受粒子数守恒约束，还受**能量守恒**（或平均能量）约束，如果你不知道每个粒子的具体位置，你应该猜它在什么状态？

1957年，Jaynes 提出：如果你选了一个熵（不确定性）不是最大的分布，这意味着你在没有证据的情况下，隐式地假设了系统“偏好”其内部的某些结构。
**最大熵原理**（MaxEnt）告诉我们：在满足约束的所有可能分布中，熵最大的那个是**最诚实**（Most Honest）、偏见最少（Least Biased）的推断。

形式化的定义与推导路径可对照：[[Shannon 熵与最大熵原则]] 与 Context 卡 [[Jaynes 1957：最大熵作为推断（信息论视角）]]。

---

## Part 3：拉格朗日乘子的机器

如何求最大熵？这是一个标准的变分问题。我们有目标函数 \(S[p]\) 和约束条件（归一化、平均能量等）。
**拉格朗日乘子法**是解决这个问题的标准机器。

神奇的是，只要约束是线性的（例如 \(\sum p_i E_i = \bar{E}\)），MaxEnt 的解永远属于**指数族（Exponential Family）**：
\[ p(x) = \frac{1}{Z} \exp\left( - \sum_k \lambda_k f_k(x) \right) \]

其中归一化常数 **\(Z\)**（配分函数）起到了至关重要的作用：
\[ Z(\lambda_1, \dots) = \sum_x \exp\left( - \sum_k \lambda_k f_k(x) \right) \]
它不仅保证了概率和为 1，更重要的是它编码了系统的全部信息。你会发现 \(-\frac{\partial \ln Z}{\partial \lambda_k}\) 正好给出了约束量的期望值。这解释了为什么在求出 \(Z\) 之后，一切热力学量（能量、熵、涨落）都能通过简单的求导得到。

这里的乘子 \(\lambda_k\) 正好对应热力学里的强度量（如 \(\beta = 1/kT\)）。这一步推导打通了数学优化与物理实在的任督二脉。

通用模板与示例推导分别见：[[最大熵推出指数族分布（通用模板）]]、[[Boltzmann 分布的最大熵推导]]。

> 这也是 MaxEnt 与正则系综的合流点：同一份信息，两个出发语言（见：[[MaxEnt 与正则系综等价（信息等价）]]；M3 会给出正则系综的物理推导）。

---

## Part 4：城市映射——OD 矩阵推断

MaxEnt 在城市科学里最著名的应用就是 **OD 矩阵推断（Origin-Destination Estimation）**。
- **微观态**：每个人具体的出发地 \(i\) 和目的地 \(j\)。
- **约束**：你只知道每个小区的出发总量 \(O_i\) 和到达总量 \(D_j\)（比如来自手机信令或地铁闸机）。
- **推断**：流量矩阵 \(T_{ij}\) 应该长什么样？

用 MaxEnt 推导，你会发现最合理的猜测是 \(T_{ij} = A_i O_i B_j D_j\)（重力模型的原型）。这不仅是模型，这是**信息甚至真理的边界**——除非你有更多数据（如各交通方式的成本约束），否则你不能假设比这更多的结构。

更细的“微观态/约束/可检验预测”映射见：[[最大熵（MaxEnt）→ OD 矩阵推断]]。

---

## 通往下一章（M2/M3）
- **M2 微正则**：当约束是“硬固定”（如总能量/总资源），MaxEnt 的自然落点就是“等概率计数”。
- **M3 正则**：当约束是“软固定”（与热库交换），指数族直接变成 Boltzmann 权重与配分函数。

## Part 5：动手时刻 (Checklist)

### 必读
- [ ] **Reading Guide**: [[Swendsen_Ch5_Probability]] (Chapter 5: 5.1-5.6)
  > 重点理解：信息缺失（Information deficiency）如何导致概率描述。
- [ ] **Seminal Paper**: 读一遍 Jaynes 1957 的 Abstract 和 Introduction。

### 习题
- [ ] **Written**: `exercises/written/E01_maxent_to_boltzmann.md`
  - 手推一遍：从 \(S\) 和 \(\langle E \rangle\) 约束导出 \(p \propto e^{-\beta E}\)。这是基本功。
- [ ] **Notebook**: `exercises/notebooks/E01_maxent_discrete_numeric.ipynb`
  - 跑通代码，试着改变约束值（平均骰子点数），看 \(\lambda\) 如何变化。

### 验收标准
- [ ] 看到“已知均值为 \(\bar{x}\)”，能脱口而出“最大熵分布是指数分布（正实数）”。
- [ ] 明白为什么 Gaussian 分布是“已知方差”时的最大熵分布（见：[[已知均值与方差的最大熵解（高斯）]]）。
- [ ] 能够解释：为什么 \(Z\)（配分函数）是乘子 \(\lambda\) 的核心（见：[[已知均值与方差的最大熵解（高斯）]]）。
